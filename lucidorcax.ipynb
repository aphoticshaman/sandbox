{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"},{"sourceId":13637997,"sourceType":"datasetVersion","datasetId":8666445}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Cell 1\n################################################################################\n#\n# ðŸŒŠâš›ï¸ LUCIDORCA ULTIMATE SOLVER - REFACTORED\n#\n# Cell 1: Imports & Environment Setup\n#\n# This cell is designed with \"backwards-planning\" (MDMP) based on our\n# 3-Phase (Heuristic, Abstraction, Reasoning) roadmap.\n#\n# We are importing all dependencies for the *entire system* up front\n# to ensure a clean, linear execution flow.\n#\n################################################################################\n\n# --- Core Python Libraries ---\nimport numpy as np\nimport json\nimport time\nimport os\nimport gc\nimport sys\nimport resource\nimport pickle\nimport re\nimport copy\nimport signal\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional, Any, Callable, Set\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, Counter, deque\nfrom enum import Enum\nfrom itertools import combinations, product\n\n# --- Phase 1: Heuristic Triage & Object Perception ---\n# scipy.ndimage.label is the S-tier replacement for our\n# custom _flood_fill_label. It is C-optimized and robust.\nfrom scipy.ndimage import label as scipy_label\n\n# --- Phase 3: Reasoning Engine (Deep Search) ---\n# Required for the true parallel search in our ReasoningSolver\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, TimeoutError as FutureTimeoutError\n\n# --- Environment & Logging Setup ---\n# Set higher recursion depth for deep symbolic search\nsys.setrecursionlimit(10000)\n\nprint(\"=\"*70)\nprint(\"ðŸŒŠâš›ï¸ LucidOrca Solver: Cell 1 Imports Loaded\")\nprint(f\"  Python Version: {sys.version.split(' ')[0]}\")\nprint(f\"  Numpy Version: {np.__version__}\")\nprint(f\"  Recursion Limit: {sys.getrecursionlimit()}\")\nprint(\"=\"*70)\n#Cell 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:41:33.885834Z","iopub.execute_input":"2025-11-08T20:41:33.886811Z","iopub.status.idle":"2025-11-08T20:41:34.445860Z","shell.execute_reply.started":"2025-11-08T20:41:33.886769Z","shell.execute_reply":"2025-11-08T20:41:34.444948Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nðŸŒŠâš›ï¸ LucidOrca Solver: Cell 1 Imports Loaded\n  Python Version: 3.11.13\n  Numpy Version: 1.26.4\n  Recursion Limit: 10000\n======================================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#Cell 2\n################################################################################\n#\n# ðŸŒŠâš›ï¸ LUCIDORCA ULTIMATE SOLVER - (LTM-v4 REBUILD)\n#\n# Cell 2: Global Configuration & Core Utilities\n#\n# *** LTM-v4 HOTFIX 19 (\"Ultra-Deep, Hyper-Pruned\" Search): ***\n# 1. Rationale: We are still getting a mix of Timeout and MaxDepth.\n#    We will trade \"pruning width\" (Cell 6) for more \"search depth\".\n# 2. `MAX_PROGRAM_DEPTH`: Increased from 15 to 20 (to solve MaxDepth).\n# 3. `BEAM_SEARCH_WIDTH`: Kept at 5 (our \"Narrow\" setting).\n#\n################################################################################\n\n# --- 1. Global Configuration ---\n\n@dataclass\nclass ChampionshipConfig:\n    \"\"\"\n    Single source of truth for all solver parameters.\n    This allows for easy tuning and validation.\n    \"\"\"\n    \n    # --- R&D / Diagnostic Mode ---\n    DIAGNOSTIC_RUN: bool = True\n    DIAGNOSTIC_SAMPLE_SIZE: int = 100 # Number of tasks for \"micro-train\"\n    \n    # --- HOTFIX 16 Knobs ---\n    DIAGNOSTIC_MIN_RUNTIME_MINUTES: float = 30.0 # 30-minute minimum\n    PUNT_TASK_BUDGET_SECONDS: float = 60.0       # 1-minute per punt task\n    \n    \n    # --- Time Management (in seconds) ---\n    total_time_budget: float = 28800.0   # 8 hours (Kaggle Hard Limit is 9)\n    submission_buffer: float = 900.0     # 15 min buffer for saving files\n    \n    # --- LTM Training Budget (HOTFIX 11) ---\n    LTM_BUDGET_PERCENT: float = 0.30     # 30% of total_time_budget\n    \n    # --- Time Allocation Ratios for 3-Phase Solving (Inference) ---\n    abstraction_pass_time_ratio: float = 0.30 # 30% of *remaining* time\n    reasoning_pass_time_ratio: float = 0.70 # 70% of *remaining* time\n\n    # --- System & Resource ---\n    parallel_workers: int = 4                # Match Kaggle's 4 CPU cores\n    kaggle_memory_gb: float = 16.0\n    memory_limit_ratio: float = 0.75         # Use 75% of 16GB = 12GB\n    max_memory_bytes: int = int(kaggle_memory_gb * memory_limit_ratio * 1024**3)\n\n    # --- LTM-v4 Solver Behavior Knobs ---\n    \n    # *** HOTFIX 19: \"Ultra-Deep, Hyper-Pruned\" ***\n    MAX_PROGRAM_DEPTH: int = 20 # Increased from 15\n    BEAM_SEARCH_WIDTH: int = 5 # Kept from HOTFIX 18\n    \n    # k-Nearest Neighbors for LTM cache query\n    LTM_CACHE_K: int = 5\n\n\n# Instantiate the global config object\nCONFIG = ChampionshipConfig()\n\nprint(\"=\"*70)\nprint(\"ðŸŒŠâš›ï¸ LucidOrca Solver: Cell 2 Configuration (LTM-v4) [HOTFIX 19]\")\nif CONFIG.DIAGNOSTIC_RUN:\n    print(\"  *** âš ï¸  DIAGNOSTIC MODE ENABLED âš ï¸ ***\")\n    print(f\"  Sample Size: {CONFIG.DIAGNOSTIC_SAMPLE_SIZE} tasks\")\n    print(f\"  Minimum Runtime: {CONFIG.DIAGNOSTIC_MIN_RUNTIME_MINUTES:.0f} minutes\")\nprint(f\"  Punt Task Budget: {CONFIG.PUNT_TASK_BUDGET_SECONDS:.0f} seconds\")\nprint(f\"  Total Time Budget: {CONFIG.total_time_budget / 3600:.2f} hours\")\nprint(f\"  LTM Training Budget: {CONFIG.LTM_BUDGET_PERCENT * 100:.0f}% of Total\")\nprint(f\"  Memory Limit: {CONFIG.max_memory_bytes / 1024**3:.2f} GB\")\nprint(f\"  Program Depth: {CONFIG.MAX_PROGRAM_DEPTH} (Actions) | Beam Width: {CONFIG.BEAM_SEARCH_WIDTH}\")\n\n\n# --- 2. Core Utility Classes ---\n\nclass TimingProfiler:\n    \"\"\"Track timing at every level: task, solver, function, operation\"\"\"\n    def __init__(self):\n        self.timings = defaultdict(list)\n        self.start_times = {}\n        self.call_counts = defaultdict(int)\n\n    def start(self, category: str):\n        self.start_times[category] = time.time()\n\n    def end(self, category: str):\n        if category in self.start_times:\n            duration = time.time() - self.start_times[category]\n            self.timings[category].append(duration)\n            self.call_counts[category] += 1\n            del self.start_times[category]\n            return duration\n        return 0.0\n\n    def get_stats(self, category: str = None):\n        if category:\n            if category in self.timings:\n                times = self.timings[category]\n                if not times: return {'count': 0, 'total': 0, 'mean': 0, 'median': 0, 'min': 0, 'max': 0}\n                return {\n                    'count': len(times),\n                    'total': sum(times),\n                    'mean': np.mean(times),\n                    'median': np.median(times),\n                    'min': min(times),\n                    'max': max(times),\n                }\n            return {}\n        return {cat: self.get_stats(cat) for cat in self.timings.keys()}\n\n    def print_summary(self, top_n: int = 20):\n        print(\"\\n\" + \"=\"*70)\n        print(\"â±ï¸  DETAILED TIMING BREAKDOWN\")\n        print(\"=\"*70)\n        \n        valid_categories = {k: sum(v) for k, v in self.timings.items() if v}\n        if not valid_categories:\n            print(\"  No timing data recorded.\")\n            print(\"=\"*70)\n            return\n\n        sorted_categories = sorted(\n            valid_categories.keys(),\n            key=lambda k: valid_categories[k],\n            reverse=True\n        )[:top_n]\n        \n        for cat in sorted_categories:\n            stats = self.get_stats(cat)\n            print(f\"  {cat:40s}: {stats['total']:7.2f}s ({stats['count']:4d} calls, \"\n                  f\"avg: {stats['mean']:.3f}s)\")\n        print(\"=\"*70)\n\nprofiler = TimingProfiler()\n\n# --- Metric Logger (HOTFIX 9) ---\n\nclass MetricLogger:\n    \"\"\"A simple, robust CSV logger for our meta-analysis.\"\"\"\n    def __init__(self, filepath: Path):\n        self.filepath = filepath\n        try:\n            self.file_handle = open(self.filepath, 'w')\n            print(f\"  âœ… MetricLogger initialized. Writing to {self.filepath}\")\n        except Exception as e:\n            print(f\"  âŒ CRITICAL: MetricLogger FAILED to open file: {e}\")\n            self.file_handle = None\n\n    def write_header(self, columns: List[str]):\n        if not self.file_handle: return\n        try:\n            print(','.join(columns), file=self.file_handle, flush=True)\n        except Exception as e:\n            print(f\"  âŒ MetricLogger Error (Header): {e}\")\n\n    def log(self, data: Dict):\n        if not self.file_handle: return\n        try:\n            str_data = [str(data.get(k, \"\")) for k in data['columns_order']]\n            print(','.join(str_data), file=self.file_handle, flush=True)\n        except Exception as e:\n            print(f\"  âŒ MetricLogger Error (Log): {e}\")\n    \n    def close(self):\n        if self.file_handle:\n            self.file_handle.close()\n            print(f\"  âœ… MetricLogger closed. Final logs saved to {self.filepath}\")\n\n\n# --- 3. Resource & Memory Utilities ---\n\ndef setup_memory_limits():\n    \"\"\"Set memory limits to % of Kaggle's kernel limit\"\"\"\n    try:\n        soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n        target_bytes = CONFIG.max_memory_bytes\n        \n        resource.setrlimit(resource.RLIMIT_AS, (target_bytes, hard))\n        \n        soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n        print(f\"ðŸ§  Memory limit set: {soft / (1024**3):.2f} GB\")\n        \n        gc.enable()\n        gc.set_threshold(700, 10, 10)\n        print(f\"â™»ï¸  Garbage collection: ENABLED (aggressive mode)\")\n        \n    except Exception as e:\n        print(f\"âš ï¸  Could not set memory limit (not on Linux?): {e}\")\n\ndef get_memory_usage() -> dict:\n    \"\"\"Get current memory usage statistics\"\"\"\n    try:\n        usage = resource.getrusage(resource.RUSAGE_SELF)\n        max_rss_gb = usage.ru_maxrss / (1024 * 1024)\n        return {'max_rss_gb': max_rss_gb, 'max_rss_mb': usage.ru_maxrss / 1024}\n    except Exception as e:\n        print(f\"âš ï¸  Could not get memory usage: {e}\")\n        return {'max_rss_gb': 0, 'max_rss_mb': 0}\n\n\n# --- 4. Difficulty & Time Allocation Heuristics ---\n\ndef estimate_task_difficulty(task: dict) -> float:\n    \"\"\"Estimate task difficulty for curriculum learning (part of Phase 1 Triage).\"\"\"\n    profiler.start(\"estimate_task_difficulty\")\n    examples = task.get('train', [])\n    if not examples:\n        profiler.end(\"estimate_task_difficulty\")\n        return 999.0\n    \n    try:\n        avg_grid_size = np.mean([\n            np.array(ex['input']).size + np.array(ex['output']).size\n            for ex in examples\n        ])\n        grid_complexity = avg_grid_size / 100.0\n\n        all_colors = set()\n        for ex in examples:\n            all_colors.update(np.array(ex['input']).flatten().tolist())\n            all_colors.update(np.array(ex['output']).flatten().tolist())\n        color_complexity = len(all_colors) * 0.5\n\n        num_examples_penalty = 10.0 / (len(examples) + 1)\n\n        shape_changes = sum(\n            1 for ex in examples\n            if np.array(ex['input']).shape != np.array(ex['output']).shape\n        )\n        shape_complexity = shape_changes * 2.0\n\n        size_ratios = []\n        for ex in examples:\n            in_size = np.array(ex['input']).size\n            out_size = np.array(ex['output']).size\n            if in_size > 0:\n                size_ratios.append(abs(out_size / in_size - 1.0))\n        size_change_complexity = np.mean(size_ratios) * 3.0 if size_ratios else 0.0\n\n        difficulty = (\n            grid_complexity +\n            color_complexity +\n            num_examples_penalty +\n            shape_complexity +\n            size_change_complexity\n        )\n        profiler.end(\"estimate_task_difficulty\")\n        return difficulty\n    \n    except Exception as e:\n        profiler.end(\"estimate_task_difficulty\")\n        return 10.0\n\nprint(\"  Core utilities (Profiler, Memory, Difficulty, MetricLogger) defined.\")\nprint(\"=\"*70)\n#Cell 2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:41:34.447388Z","iopub.execute_input":"2025-11-08T20:41:34.447843Z","iopub.status.idle":"2025-11-08T20:41:34.479604Z","shell.execute_reply.started":"2025-11-08T20:41:34.447817Z","shell.execute_reply":"2025-11-08T20:41:34.478337Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nðŸŒŠâš›ï¸ LucidOrca Solver: Cell 2 Configuration (LTM-v4) [HOTFIX 19]\n  *** âš ï¸  DIAGNOSTIC MODE ENABLED âš ï¸ ***\n  Sample Size: 100 tasks\n  Minimum Runtime: 30 minutes\n  Punt Task Budget: 60 seconds\n  Total Time Budget: 8.00 hours\n  LTM Training Budget: 30% of Total\n  Memory Limit: 12.00 GB\n  Program Depth: 20 (Actions) | Beam Width: 5\n  Core utilities (Profiler, Memory, Difficulty, MetricLogger) defined.\n======================================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#Cell 3\n################################################################################\n#\n# ðŸŒŠâš›ï¸ LUCIDORCA ULTIMATE SOLVER - (LTM-v4 REBUILD)\n#\n# Cell 3: The Perception Engine & Atomic Primitives\n#\n# *** LTM-v4 REFACTOR: ***\n# 1. This cell's primary responsibility is now PERCEPTION.\n# 2. `HyperFeatureObjectClustering` (from old Cell 5/7) is promoted\n#    to be the core `PerceptionEngine`.\n# 3. `HyperObject` (the \"noun\" of our AGI) is now defined here.\n# 4. DEPRECATED: `find_abstraction_rule` is REMOVED. It is a failed model.\n# 5. The old primitives (e.g., `rot90`) are re-classified as\n#    `ATOMIC_PRIMITIVES`. They are \"building blocks\" for the\n#    new meta-primitives (Cell 4), not solvers in themselves.\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(\"ðŸŒŠâš›ï¸ LucidOrca Solver: Cell 3 (LTM-v4) Perception Engine\")\n\n# --- 1. The Core \"Noun\" of our AGI ---\n\n@dataclass\nclass HyperObject:\n    \"\"\"\n    A \"noun\" with advanced features, used by all LTM-v4+ systems.\n    This dataclass is the primary output of the PerceptionEngine.\n    \"\"\"\n    obj_id: int\n    color: int\n    size: int\n    positions: np.ndarray  # (N, 2) array of (r, c) coordinates\n    bbox: Tuple[int, int, int, int] # (min_r, min_c, max_r, max_c)\n    center: Tuple[float, float]     # (mean_r, mean_c)\n    \n    # Advanced features (from LTM-v2)\n    symmetry_score: float = 0.0\n    density: float = 0.0\n    hierarchy_level: int = 0\n    topology: str = \"simple\" # 'simple', 'hollow', etc.\n\nprint(\"  Defined: HyperObject (The AGI's 'Noun')\")\n\n\n# --- 2. The Core \"Vision System\" of our AGI ---\n\nclass PerceptionEngine:\n    \"\"\"\n    This is the core LTM-v4 \"Vision System.\"\n    It replaces and merges all previous perception modules.\n    Its job is to take a raw grid and transform it into a\n    structured list of HyperObjects.\n    \n    This is the foundation for Blind Spot #1 (Salience) and #2 (Multi-Modal).\n    \"\"\"\n    def __init__(self):\n        print(\"  PerceptionEngine (LTM-v4 'Vision') initialized.\")\n        # This class is stateless for now\n        pass\n\n    def analyze(self, grid: np.ndarray) -> List[HyperObject]:\n        \"\"\"\n        Analyzes a grid and returns a list of all found HyperObjects.\n        \"\"\"\n        profiler.start(\"PerceptionEngine.analyze\")\n        \n        # Use scipy.ndimage.label for C-optimized object finding\n        basic_objects = self._extract_basic_objects(grid)\n        if not basic_objects:\n            profiler.end(\"PerceptionEngine.analyze\")\n            return []\n\n        # Compute advanced features for each object\n        hyper_objects = [self._compute_hyper_features(obj, i, grid) \n                         for i, obj in enumerate(basic_objects)]\n        \n        # Compute relational features (e.g., \"is_inside\")\n        self._compute_hierarchy(hyper_objects)\n        \n        profiler.end(\"PerceptionEngine.analyze\")\n        return hyper_objects\n\n    def _extract_basic_objects(self, grid: np.ndarray) -> List[Dict]:\n        \"\"\"\n        Uses scipy.ndimage.label to find all connected components (\"blobs\").\n        \"\"\"\n        objects = []\n        if grid.size == 0:\n            return objects\n        \n        unique_colors = np.unique(grid)\n        for color in unique_colors:\n            if color == 0: continue # Ignore background\n            \n            color_mask = (grid == color)\n            labeled_array, num_features = scipy_label(color_mask)\n\n            for obj_id in range(1, num_features + 1):\n                obj_mask = (labeled_array == obj_id)\n                positions = np.argwhere(obj_mask)\n                if positions.size == 0: continue\n                \n                min_row, min_col = positions.min(axis=0)\n                max_row, max_col = positions.max(axis=0)\n\n                objects.append({\n                    'color': int(color),\n                    'size': len(positions),\n                    'positions': positions,\n                    'mask': obj_mask,\n                    'bbox': (min_row, min_col, max_row, max_col),\n                    'center': (positions[:, 0].mean(), positions[:, 1].mean()),\n                })\n        return objects\n\n    def _compute_hyper_features(self, obj: Dict, obj_id: int, grid: np.ndarray) -> HyperObject:\n        \"\"\"\n        Upgrades a \"basic object\" dict to a \"HyperObject\" dataclass\n        by computing advanced features (density, symmetry, etc.).\n        \"\"\"\n        min_r, min_c, max_r, max_c = obj['bbox']\n        bbox_area = (max_r - min_r + 1) * (max_c - min_c + 1)\n        \n        # Get the snippet of the grid corresponding to the object\n        obj_region = grid[min_r:max_r+1, min_c:max_c+1]\n        obj_mask_local = obj['mask'][min_r:max_r+1, min_c:max_c+1]\n        obj_grid_snippet = obj_region * obj_mask_local\n\n        # Calculate local symmetry\n        symmetry_h = np.mean(obj_grid_snippet == np.fliplr(obj_grid_snippet)) if obj_grid_snippet.size > 0 else 0.0\n        symmetry_v = np.mean(obj_grid_snippet == np.flipud(obj_grid_snippet)) if obj_grid_snippet.size > 0 else 0.0\n        \n        density = obj['size'] / max(bbox_area, 1)\n        \n        return HyperObject(\n            obj_id=obj_id,\n            color=obj['color'],\n            positions=obj['positions'],\n            size=obj['size'],\n            bbox=obj['bbox'],\n            center=obj['center'],\n            symmetry_score=(symmetry_h + symmetry_v) / 2.0,\n            density=density,\n            topology=\"hollow\" if bbox_area > 0 and density < 0.5 and density > 0.1 else \"simple\"\n        )\n\n    def _compute_hierarchy(self, objects: List[HyperObject]):\n        \"\"\"\n        Calculates relational features, like \"which object is inside which\".\n        Modifies objects in-place.\n        \"\"\"\n        for i, obj_i in enumerate(objects):\n            level = 0\n            for j, obj_j in enumerate(objects):\n                if i == j: continue\n                \n                # Is obj_i's center inside obj_j's bbox?\n                min_r, min_c, max_r, max_c = obj_j.bbox\n                cy, cx = obj_i.center\n                if min_r < cy < max_r and min_c < cx < max_c:\n                    # More advanced check: is it *truly* contained?\n                    # For now, a simple bbox check is a good heuristic.\n                    level += 1\n            obj_i.hierarchy_level = level\n\nprint(\"  Defined: PerceptionEngine (Core LTM-v4 'Vision' System)\")\n\n\n# --- 3. Atomic Primitive Dictionaries (\"The Building Blocks\") ---\n# These are the \"legacy\" grid-level ops. They are no longer\n# solvers, but are the instruction set for our LTM-v5 \"Math Brain\"\n# and the components of our LTM-v4 \"Meta-Primitives\" (Cell 4).\n\n# Geometric primitives\nATOMIC_PRIMITIVES_GEOMETRIC = {\n    'identity': lambda g: g,\n    'rot90': lambda g: np.rot90(g, 1),\n    'rot180': lambda g: np.rot90(g, 2),\n    'rot270': lambda g: np.rot90(g, 3),\n    'flip_h': lambda g: np.fliplr(g), # Flip horizontal (left-right)\n    'flip_v': lambda g: np.flipud(g), # Flip vertical (up-down)\n    'transpose': lambda g: g.T,\n    'anti_transpose': lambda g: np.rot90(g.T, 2),\n}\n\n# Scaling primitives\nATOMIC_PRIMITIVES_SCALING = {\n    'tile_2x2': lambda g: np.tile(g, (2, 2)),\n    'tile_2x1': lambda g: np.tile(g, (2, 1)),\n    'tile_1x2': lambda g: np.tile(g, (1, 2)),\n    'tile_3x3': lambda g: np.tile(g, (3, 3)),\n}\n\n# Color primitives\nATOMIC_PRIMITIVES_COLOR = {\n    'invert_colors_mod10': lambda g: (9 - g) % 10,\n    'increment_colors_mod10': lambda g: (g + 1) % 10,\n    'decrement_colors_mod10': lambda g: (g - 1) % 10,\n    'mask_nonzero': lambda g: (g > 0).astype(int),\n    'extract_color_1': lambda g: (g == 1).astype(int),\n    'extract_color_2': lambda g: (g == 2).astype(int),\n    'extract_color_3': lambda g: (g == 3).astype(int),\n    'extract_color_4': lambda g: (g == 4).astype(int),\n    'extract_color_5': lambda g: (g == 5).astype(int),\n    'extract_color_6': lambda g: (g == 6).astype(int),\n    'extract_color_7': lambda g: (g == 7).astype(int),\n    'extract_color_8': lambda g: (g == 8).astype(int),\n}\n\n# Spatial & Morphological primitives\nATOMIC_PRIMITIVES_SPATIAL = {\n    'center_crop_1px': lambda g: g[1:-1, 1:-1] if g.shape[0] > 2 and g.shape[1] > 2 else g,\n    'border_pad_1px_zero': lambda g: np.pad(g, 1, mode='constant', constant_values=0),\n    'extract_edges': lambda g: (np.abs(g - np.roll(g, 1, axis=0)) + \n                                np.abs(g - np.roll(g, 1, axis=1))) > 0,\n}\n\n# Aggregation/Value primitives (output grid is a single value)\nATOMIC_PRIMITIVES_AGGREGATION = {\n    'count_unique_colors': lambda g: np.array([[len(np.unique(g[g > 0]))]]),\n    'count_pixels': lambda g: np.array([[np.sum(g > 0)]]),\n    'get_max_color': lambda g: np.array([[np.max(g)]]),\n}\n\n# All \"atomic\" primitives combined\nALL_ATOMIC_PRIMITIVES = {\n    **ATOMIC_PRIMITIVES_GEOMETRIC,\n    **ATOMIC_PRIMITIVES_SCALING,\n    **ATOMIC_PRIMITIVES_COLOR,\n    **ATOMIC_PRIMITIVES_SPATIAL,\n    **ATOMIC_PRIMITIVES_AGGREGATION,\n}\n\n# Pre-compute the list of (name, function) tuples for fast iteration\natomic_primitives_to_test = list(ALL_ATOMIC_PRIMITIVES.items())\n\nprint(f\"  Defined: {len(ALL_ATOMIC_PRIMITIVES)} 'Atomic Primitives' (Building Blocks)\")\nprint(\"=\"*70)\n#Cell 3\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:41:34.480627Z","iopub.execute_input":"2025-11-08T20:41:34.480913Z","iopub.status.idle":"2025-11-08T20:41:34.511659Z","shell.execute_reply.started":"2025-11-08T20:41:34.480892Z","shell.execute_reply":"2025-11-08T20:41:34.510667Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nðŸŒŠâš›ï¸ LucidOrca Solver: Cell 3 (LTM-v4) Perception Engine\n  Defined: HyperObject (The AGI's 'Noun')\n  Defined: PerceptionEngine (Core LTM-v4 'Vision' System)\n  Defined: 30 'Atomic Primitives' (Building Blocks)\n======================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#Cell 4\n################################################################################\n#\n# ðŸŒŠâš›ï¸ LUCIDORCA ULTIMATE SOLVER - (LTM-v4 REBUILD)\n#\n# Cell 4: The Meta-Primitive Instruction Set (The \"ALU\")\n#\n# *** LTM-v4 HOTFIX 15 (HPN Prereq): ***\n# 1. This is the \"full-power\" ALU, including `draw_path` (from HOTFIX 12),\n#    which our new HPN-guided synthesizer will now learn to use\n#    intelligently instead of by \"flailing\".\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(\"ðŸŒŠâš›ï¸ LucidOrca Solver: Cell 4 (LTM-v4 HOTFIX 15) Meta-Primitive Set\")\n\n# --- 1. The LTM-v4 \"Instruction Set Architecture\" (ISA) ---\n\nclass MetaPrimitives:\n    \"\"\"\n    Container for our new \"Object-Aware\" (LTM-v4) primitive instruction set.\n    \"\"\"\n    \n    def __init__(self, perception_engine: PerceptionEngine):\n        \"\"\"\n        The Meta-Primitive set *requires* a PerceptionEngine (from Cell 3)\n        to \"see\" the objects it will act upon.\n        \"\"\"\n        self.perception_engine = perception_engine\n        print(\"  MetaPrimitives (LTM-v4 'ALU') initialized. [HOTFIX 15]\")\n\n    # --- \"Finder\" / \"Selector\" Primitives (The \"Nouns\") ---\n    \n    def find_objects(self, ctx: Dict, grid: np.ndarray, **params) -> Tuple[Dict, np.ndarray]:\n        \"\"\"\n        Stores List[HyperObject] in `ctx['last_result']`.\n        Returns (new_ctx, original_grid).\n        \"\"\"\n        profiler.start(\"Primitive.find_objects\")\n        all_objects = self.perception_engine.analyze(grid)\n        filtered_objects = all_objects\n        \n        if 'color' in params:\n            filtered_objects = [o for o in filtered_objects if o.color == params['color']]\n        if 'size' in params:\n            filtered_objects = [o for o in filtered_objects if o.size == params['size']]\n        if 'min_size' in params:\n            filtered_objects = [o for o in filtered_objects if o.size >= params['min_size']]\n        if 'max_size' in params:\n            filtered_objects = [o for o in filtered_objects if o.size <= params['max_size']]\n        if 'topology' in params:\n            filtered_objects = [o for o in filtered_objects if o.topology == params['topology']]\n        if 'hierarchy' in params:\n            filtered_objects = [o for o in filtered_objects if o.hierarchy_level == params['hierarchy']]\n            \n        ctx['last_result'] = filtered_objects\n        profiler.end(\"Primitive.find_objects\")\n        return ctx, grid # Return original grid\n\n    def get_largest(self, ctx: Dict, grid: np.ndarray, **params) -> Tuple[Dict, np.ndarray]:\n        \"\"\"Takes a list of objects from `ctx` and returns only the largest.\"\"\"\n        target_key = params.get('target', 'last_result')\n        objects = ctx.get(target_key, [])\n        if not objects or not isinstance(objects, list):\n            ctx['last_result'] = []\n            return ctx, grid\n            \n        max_size = max(o.size for o in objects)\n        ctx['last_result'] = [o for o in objects if o.size == max_size]\n        return ctx, grid\n\n    def get_smallest(self, ctx: Dict, grid: np.ndarray, **params) -> Tuple[Dict, np.ndarray]:\n        \"\"\"Takes a list of objects from `ctx` and returns only the smallest.\"\"\"\n        target_key = params.get('target', 'last_result')\n        objects = ctx.get(target_key, [])\n        if not objects or not isinstance(objects, list):\n            ctx['last_result'] = []\n            return ctx, grid\n            \n        min_size = min(o.size for o in objects)\n        ctx['last_result'] = [o for o in objects if o.size == min_size]\n        return ctx, grid\n\n\n    # --- \"Transformer\" / \"Action\" Primitives (The \"Verbs\") ---\n    \n    def move(self, ctx: Dict, grid: np.ndarray, **params) -> Tuple[Dict, np.ndarray]:\n        \"\"\"\n        Moves a list of objects (from `ctx`) by a given `delta` (dr, dc).\n        Returns (new_ctx, new_grid).\n        \"\"\"\n        profiler.start(\"Primitive.move\")\n        target_key = params.get('target', 'last_result')\n        delta = params.get('delta', (0, 0))\n        dr, dc = delta\n        objects_to_move = ctx.get(target_key, [])\n        \n        if not objects_to_move or not isinstance(objects_to_move, list):\n            profiler.end(\"Primitive.move\")\n            return ctx, grid\n            \n        moved_pixels = set()\n        for obj in objects_to_move:\n            if isinstance(obj, HyperObject):\n                for r, c in obj.positions:\n                    moved_pixels.add((r, c))\n\n        new_grid = np.zeros_like(grid)\n        rows, cols = grid.shape\n        \n        for r in range(rows):\n            for c in range(cols):\n                if (r, c) not in moved_pixels:\n                    new_grid[r, c] = grid[r, c]\n                    \n        for obj in objects_to_move:\n            if isinstance(obj, HyperObject):\n                for r, c in obj.positions:\n                    new_r, new_c = r + dr, c + dc\n                    if 0 <= new_r < rows and 0 <= new_c < cols:\n                        new_grid[new_r, new_c] = obj.color\n        \n        ctx['last_result'] = new_grid\n        profiler.end(\"Primitive.move\")\n        return ctx, new_grid\n\n    def delete(self, ctx: Dict, grid: np.ndarray, **params) -> Tuple[Dict, np.ndarray]:\n        \"\"\"\n        Deletes a list of objects (from `ctx`) from the grid.\n        Returns (new_ctx, new_grid).\n        \"\"\"\n        profiler.start(\"Primitive.delete\")\n        target_key = params.get('target', 'last_result')\n        objects_to_delete = ctx.get(target_key, [])\n        \n        if not objects_to_delete or not isinstance(objects_to_delete, list):\n            profiler.end(\"Primitive.delete\")\n            return ctx, grid\n            \n        new_grid = grid.copy()\n        for obj in objects_to_delete:\n             if isinstance(obj, HyperObject):\n                for r, c in obj.positions:\n                    new_grid[r, c] = 0\n        \n        ctx['last_result'] = new_grid\n        profiler.end(\"Primitive.delete\")\n        return ctx, new_grid\n\n    def recolor(self, ctx: Dict, grid: np.ndarray, **params) -> Tuple[Dict, np.ndarray]:\n        \"\"\"\n        Changes the color of a list of objects (from `ctx`) to a `new_color`.\n        Returns (new_ctx, new_grid).\n        \"\"\"\n        profiler.start(\"Primitive.recolor\")\n        target_key = params.get('target', 'last_result')\n        new_color = params.get('color', 0)\n        objects_to_recolor = ctx.get(target_key, [])\n        \n        if not objects_to_recolor or not isinstance(objects_to_recolor, list):\n            profiler.end(\"Primitive.recolor\")\n            return ctx, grid\n            \n        new_grid = grid.copy()\n        for obj in objects_to_recolor:\n            if isinstance(obj, HyperObject):\n                for r, c in obj.positions:\n                    new_grid[r, c] = new_color\n        \n        ctx['last_result'] = new_grid\n        profiler.end(\"Primitive.recolor\")\n        return ctx, new_grid\n    \n    def copy_objects(self, ctx: Dict, grid: np.ndarray, **params) -> Tuple[Dict, np.ndarray]:\n        \"\"\"\n        Copies a list of objects (from `ctx`) by a given `delta` (dr, dc).\n        The original objects are *kept* in place.\n        Returns (new_ctx, new_grid).\n        \"\"\"\n        profiler.start(\"Primitive.copy_objects\")\n        \n        target_key = params.get('target', 'last_result')\n        delta = params.get('delta', (0, 0))\n        dr, dc = delta\n        \n        objects_to_copy = ctx.get(target_key, [])\n        if not objects_to_copy or not isinstance(objects_to_copy, list):\n            profiler.end(\"Primitive.copy_objects\")\n            return ctx, grid\n            \n        new_grid = grid.copy()\n        rows, cols = grid.shape\n                    \n        for obj in objects_to_copy:\n             if isinstance(obj, HyperObject):\n                for r, c in obj.positions:\n                    new_r, new_c = r + dr, c + dc\n                    if 0 <= new_r < rows and 0 <= new_c < cols:\n                        new_grid[new_r, new_c] = obj.color\n        \n        ctx['last_result'] = new_grid\n        profiler.end(\"Primitive.copy_objects\")\n        return ctx, new_grid\n\n    \n    # --- Drawing Primitive (from HOTFIX 12) ---\n    \n    def _bresenham_line(self, grid: np.ndarray, p1: Tuple[int, int], \n                        p2: Tuple[int, int], color: int):\n        \"\"\"\n        Internal helper. Draws a line on `grid` in-place.\n        p1, p2 are (row, col) tuples.\n        \"\"\"\n        x0, y0 = p1[1], p1[0] # (col, row)\n        x1, y1 = p2[1], p2[0] # (col, row)\n        rows, cols = grid.shape\n\n        dx = abs(x1 - x0)\n        dy = -abs(y1 - y0)\n        sx = 1 if x0 < x1 else -1\n        sy = 1 if y0 < y1 else -1\n        err = dx + dy\n        \n        while True:\n            if 0 <= y0 < rows and 0 <= x0 < cols:\n                grid[y0, x0] = color\n            if x0 == x1 and y0 == y1:\n                break\n            e2 = 2 * err\n            if e2 >= dy:\n                err += dy\n                x0 += sx\n            if e2 <= dx:\n                err += dx\n                y0 += sy\n\n    def draw_path(self, ctx: Dict, grid: np.ndarray, **params) -> Tuple[Dict, np.ndarray]:\n        \"\"\"\n        Takes a list of objects, sorts them by y-coordinate, and draws\n        a path connecting their centers.\n        \"\"\"\n        profiler.start(\"Primitive.draw_path\")\n        \n        target_key = params.get('target', 'all_objects')\n        color = params.get('color', 1)\n        if color == 0: color = 1\n            \n        objects_to_connect = ctx.get(target_key, [])\n        if not objects_to_connect or not isinstance(objects_to_connect, list) or len(objects_to_connect) < 2:\n            profiler.end(\"Primitive.draw_path\")\n            return ctx, grid\n            \n        new_grid = grid.copy()\n        \n        sorted_objects = sorted(objects_to_connect, key=lambda o: (o.center[0], o.center[1]))\n        \n        for i in range(len(sorted_objects) - 1):\n            p1 = (int(round(sorted_objects[i].center[0])), int(round(sorted_objects[i].center[1])))\n            p2 = (int(round(sorted_objects[i+1].center[0])), int(round(sorted_objects[i+1].center[1])))\n            \n            self._bresenham_line(new_grid, p1, p2, color)\n            \n        ctx['last_result'] = new_grid\n        profiler.end(\"Primitive.draw_path\")\n        return ctx, new_grid\n\n    \nprint(\"  Defined: MetaPrimitives (The LTM-v4 'ALU' / Instruction Set) [HOTFIX 15]\")\nprint(\"=\"*70)\n#Cell 4\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:41:34.513834Z","iopub.execute_input":"2025-11-08T20:41:34.514140Z","iopub.status.idle":"2025-11-08T20:41:34.549533Z","shell.execute_reply.started":"2025-11-08T20:41:34.514117Z","shell.execute_reply":"2025-11-08T20:41:34.548480Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nðŸŒŠâš›ï¸ LucidOrca Solver: Cell 4 (LTM-v4 HOTFIX 15) Meta-Primitive Set\n  Defined: MetaPrimitives (The LTM-v4 'ALU' / Instruction Set) [HOTFIX 15]\n======================================================================\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#Cell 5\n################################################################################\n#\n# ðŸŒŠâš›ï¸ LUCIDORCA ULTIMATE SOLVER - (LTM-v4 REBUILD)\n#\n# Cell 5: Cognitive Fingerprinting & Triage Engine\n#\n# *** LTM-v4 HOTFIX 11 (Punt Strategy Prereq): ***\n# 1. `TaskProfile` dataclass: Added `difficulty_score: float` field.\n# 2. `TaskAnalyzer.analyze`: Now saves the raw difficulty score to the\n#    profile. This is required by Cell 9 to sort tasks and find the\n#    9 \"easiest\" targets for our 2x budget \"punt\".\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(\"ðŸŒŠâš›ï¸ LucidOrca Solver: Cell 5 (LTM-v4) Fingerprinting & Triage [HOTFIX 11]\")\n\n# --- 1. The \"Task Passport\" Dataclass ---\n\n@dataclass\nclass TaskProfile:\n    \"\"\"\n    A \"passport\" for each task, generated by the TaskAnalyzer in Phase 1.\n    It holds all metadata needed for solving, including the LTM-v4 \"probe\".\n    \"\"\"\n    task_id: str\n    difficulty_tier: str  # 'easy', 'medium', 'hard'\n    basin: str            # 'rotation', 'color_mapping', 'scaling', 'unknown'\n    \n    # --- The LTM-v4 \"Probe\" ---\n    delta_fingerprint: Optional[np.ndarray] = None\n    \n    # --- HOTFIX 11: Store raw score for sorting ---\n    difficulty_score: float = 0.0\n\nprint(\"  Defined: TaskProfile (The AGI's 'Task Passport')\")\n\n\n# --- 2. The Core Grid-to-Vector Encoder ---\n\nclass VisionModelEncoder:\n    \"\"\"\n    An \"unrolled\" vision model. It extracts statistical and object-based\n    features from a single grid and encodes them into a normalized\n    1D numpy vector (the \"fingerprint\").\n    \"\"\"\n    \n    def __init__(self, perception_engine: PerceptionEngine):\n        \"\"\"\n        The encoder *requires* the PerceptionEngine from Cell 3 to \"see\"\n        objects and calculate object-based statistics.\n        \"\"\"\n        self.perception_engine = perception_engine\n        self.zero_vector = np.zeros(self.get_vector_dim())\n        print(f\"  VisionModelEncoder (LTM-v4) initialized. Fingerprint dim: {self.get_vector_dim()}\")\n\n    def get_vector_dim(self) -> int:\n        \"\"\"Returns the total dimension of our feature vector.\"\"\"\n        # shape(2) + color_hist(10) + obj_stats(4) + symmetry(3) + \n        # patterns(4) + layout(4) + complexity(1) = 28 dimensions\n        return 28\n\n    def encode_grid_to_vector(self, grid: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Encodes a single grid into a normalized 1D numpy vector (\"fingerprint\").\n        \"\"\"\n        profiler.start(\"VisionModelEncoder.encode_grid_to_vector\")\n        \n        if grid.size == 0:\n            profiler.end(\"VisionModelEncoder.encode_grid_to_vector\")\n            return self.zero_vector \n            \n        edge_density_val = self._compute_edge_density(grid)\n        \n        objects = self.perception_engine.analyze(grid)\n        obj_stats = self._compute_object_stats(objects, grid.size)\n        \n        shape_norm = np.array([grid.shape[0], grid.shape[1]]) / 30.0\n        color_hist = np.bincount(grid.flatten(), minlength=10) / max(1.0, grid.size)\n        edge_density = np.array([edge_density_val])\n        symmetry_vec = self._detect_symmetry(grid, as_vector=True)\n        patterns_vec = self._match_patterns(grid, as_vector=True)\n        layout_vec = self._analyze_layout(grid, as_vector=True)\n        complexity = np.array([self._compute_complexity(grid, edge_density_val, obj_stats[0])])\n\n        vector = np.concatenate([\n            shape_norm,      # 2 dims\n            color_hist,      # 10 dims\n            obj_stats,       # 4 dims\n            symmetry_vec,    # 3 dims\n            patterns_vec,    # 4 dims\n            layout_vec,      # 4 dims\n            complexity       # 1 dim\n        ])\n        \n        profiler.end(\"VisionModelEncoder.encode_grid_to_vector\")\n        return np.nan_to_num(vector, nan=0.0, posinf=0.0, neginf=0.0)\n\n    # --- ENCODER HELPER METHODS ---\n\n    def _compute_object_stats(self, objects: List[HyperObject], grid_size: int) -> np.ndarray:\n        \"\"\"Calculates normalized statistics about the object population.\"\"\"\n        if not objects:\n            return np.zeros(4) # count, avg_size, avg_density, num_colors\n            \n        obj_count = len(objects)\n        avg_size = np.mean([o.size for o in objects])\n        avg_density = np.mean([o.density for o in objects])\n        num_colors = len(set(o.color for o in objects))\n        \n        norm_obj_count = min(obj_count / 100.0, 1.0)\n        norm_avg_size = min(avg_size / (grid_size + 1e-6), 1.0)\n        norm_avg_density = avg_density\n        norm_num_colors = min(num_colors / 10.0, 1.0)\n        \n        return np.array([norm_obj_count, norm_avg_size, norm_avg_density, norm_num_colors])\n\n    @staticmethod\n    def _compute_edge_density(grid: np.ndarray) -> float:\n        if grid.size == 0: return 0.0\n        h_edges = np.sum(np.abs(np.diff(grid, axis=0)))\n        v_edges = np.sum(np.abs(np.diff(grid, axis=1)))\n        return (h_edges + v_edges) / max(grid.size * 9, 1) \n\n    @staticmethod\n    def _detect_symmetry(grid: np.ndarray, as_vector: bool = False) -> Any:\n        if grid.size == 0: \n            return np.zeros(3) if as_vector else []\n            \n        symmetries = []\n        is_h, is_v, is_d = 0.0, 0.0, 0.0\n        try:\n            if np.array_equal(grid, np.flipud(grid)): \n                symmetries.append('horizontal'); is_h = 1.0\n            if np.array_equal(grid, np.fliplr(grid)): \n                symmetries.append('vertical'); is_v = 1.0\n            if grid.shape[0] == grid.shape[1]:\n                if np.array_equal(grid, grid.T): \n                    symmetries.append('diag_main'); is_d = 1.0\n        except Exception: pass\n        \n        return np.array([is_h, is_v, is_d]) if as_vector else symmetries\n\n    def _match_patterns(self, grid: np.ndarray, as_vector: bool = False) -> Any:\n        if grid.size == 0:\n            return np.zeros(4) if as_vector else []\n            \n        patterns = []\n        is_stripe, is_grid_p, is_binary, is_sparse = 0.0, 0.0, 0.0, 0.0\n        try:\n            if self._is_stripe_pattern(grid): \n                patterns.append('stripe'); is_stripe = 1.0\n            if self._is_grid_pattern(grid): \n                patterns.append('grid'); is_grid_p = 1.0\n            if len(np.unique(grid)) <= 2: \n                patterns.append('binary_color'); is_binary = 1.0\n            if np.sum(grid == 0) > grid.size * 0.8: \n                patterns.append('sparse'); is_sparse = 1.0\n        except Exception: pass\n        \n        return np.array([is_stripe, is_grid_p, is_binary, is_sparse]) if as_vector else patterns\n\n    @staticmethod\n    def _is_stripe_pattern(grid: np.ndarray) -> bool:\n        if grid.shape[0] < 2 or grid.shape[1] < 2: return False\n        h_stripe = all(len(np.unique(row)) == 1 for row in grid)\n        v_stripe = all(len(np.unique(col)) == 1 for col in grid.T)\n        return h_stripe or v_stripe\n\n    @staticmethod\n    def _is_grid_pattern(grid: np.ndarray) -> bool:\n        if grid.size < 4: return False\n        nonzero = np.argwhere(grid != 0)\n        rows, cols = nonzero[:, 0], nonzero[:, 1]\n        row_diffs, col_diffs = np.diff(np.sort(np.unique(rows))), np.diff(np.sort(np.unique(cols)))\n        row_regular = (len(row_diffs) > 0) and (len(np.unique(row_diffs)) == 1)\n        col_regular = (len(col_diffs) > 0) and (len(np.unique(col_diffs)) == 1)\n        return row_regular and col_regular\n\n    @staticmethod\n    def _analyze_layout(grid: np.ndarray, as_vector: bool = False) -> Any:\n        is_empty, is_centered, is_scattered, is_distributed = 0.0, 0.0, 0.0, 1.0\n        layout_str = 'distributed'\n\n        if grid.size == 0: \n            is_empty, is_distributed = 1.0, 0.0\n            layout_str = 'empty'\n        else:\n            nonzero = np.argwhere(grid != 0)\n            if len(nonzero) == 0:\n                is_empty, is_distributed = 1.0, 0.0\n                layout_str = 'empty'\n            else:\n                centroid = nonzero.mean(axis=0)\n                center_h, center_w = (grid.shape[0] - 1) / 2, (grid.shape[1] - 1) / 2\n                dist = np.linalg.norm(centroid - np.array([center_h, center_w]))\n                spread = nonzero.std(axis=0).mean()\n                \n                max_dim = max(grid.shape[0], grid.shape[1], 1.0)\n                \n                if dist < max_dim * 0.2 and spread < max_dim * 0.3:\n                    is_centered, is_distributed = 1.0, 0.0\n                    layout_str = 'centered'\n                elif spread > max_dim * 0.4:\n                    is_scattered, is_distributed = 1.0, 0.0\n                    layout_str = 'scattered'\n\n        return np.array([is_empty, is_centered, is_scattered, is_distributed]) if as_vector else layout_str\n\n    @staticmethod\n    def _compute_complexity(grid: np.ndarray, edge_density: float, obj_count: float) -> float:\n        \"\"\"Computes a single 'complexity' score [0, 1].\"\"\"\n        if grid.size == 0: return 0.0\n        \n        flat = grid.flatten()\n        _, counts = np.unique(flat, return_counts=True)\n        probs = counts / len(flat)\n        entropy = -np.sum(probs * np.log2(probs + 1e-10))\n        max_entropy = np.log2(len(counts)) if len(counts) > 1 else 1\n        entropy_normalized = entropy / max(1, max_entropy)\n        \n        return (edge_density * 0.3) + (obj_count * 0.3) + (entropy_normalized * 0.4)\n\nprint(\"  Defined: VisionModelEncoder (Grid-to-Vector Engine)\")\n\n\n# --- 3. The Task-to-Delta-Vector Encoder ---\n\nclass TaskFingerprinter:\n    \"\"\"\n    Computes the \"delta-fingerprint\" for an entire task.\n    This vector represents the *abstract transformation* of the task\n    (i.e., v_output - v_input).\n    \"\"\"\n    def __init__(self, vision_encoder: VisionModelEncoder):\n        self.encoder = vision_encoder\n        self.zero_vector = self.encoder.zero_vector\n        print(\"  TaskFingerprinter (Task-to-Delta-Vector Engine) initialized.\")\n\n    def fingerprint(self, task: Dict) -> np.ndarray:\n        \"\"\"\n        Computes the average delta-vector (v_output - v_input)\n        across all training examples for a task.\n        \"\"\"\n        profiler.start(\"TaskFingerprinter.fingerprint\")\n        examples = task.get('train', [])\n        if not examples:\n            profiler.end(\"TaskFingerprinter.fingerprint\")\n            return self.zero_vector\n\n        delta_vectors = []\n        for ex in examples:\n            try:\n                inp_grid = np.array(ex['input'])\n                out_grid = np.array(ex['output'])\n                \n                v_in = self.encoder.encode_grid_to_vector(inp_grid)\n                v_out = self.encoder.encode_grid_to_vector(out_grid)\n                \n                v_delta = v_out - v_in\n                delta_vectors.append(v_delta)\n            \n            except Exception:\n                continue \n        \n        if not delta_vectors:\n            profiler.end(\"TaskFingerprinter.fingerprint\")\n            return self.zero_vector\n            \n        mean_delta = np.mean(delta_vectors, axis=0)\n        profiler.end(\"TaskFingerprinter.fingerprint\")\n        return mean_delta\n\nprint(\"  Defined: TaskFingerprinter (The 'Probe' Generator)\")\n\n\n# --- 4. The Phase 1 Triage Engine ---\n\nclass TaskAnalyzer:\n    \"\"\"\n    This is the complete Phase 1 \"Heuristic Triage\" engine.\n    Its job is to run a *fast, non-solving* analysis on every task\n    to generate a TaskProfile.\n    \"\"\"\n    def __init__(self, perception_engine: PerceptionEngine):\n        self.encoder = VisionModelEncoder(perception_engine)\n        self.fingerprinter = TaskFingerprinter(self.encoder)\n        print(\"  TaskAnalyzer (Phase 1 Triage Engine) initialized.\")\n\n    def analyze(self, task: Dict, task_id: str) -> TaskProfile:\n        \"\"\"\n        Runs all heuristic analyses on a single task and returns its profile.\n        \n        *** HOTFIX 11: Now saves the `difficulty_score`. ***\n        \"\"\"\n        profiler.start(f\"TaskAnalyzer.analyze.{task_id}\")\n        \n        # 1. Get Difficulty Score (from Cell 2)\n        difficulty_score = estimate_task_difficulty(task)\n        if difficulty_score < 7.0:\n            tier = 'easy'\n        elif difficulty_score < 18.0:\n            tier = 'medium'\n        else:\n            tier = 'hard'\n\n        # 2. Detect Attractor Basin (Task Type)\n        basin = self._detect_basin(task.get('train', []))\n        \n        # 3. --- LTM-v4 STEP: Generate Delta-Fingerprint ---\n        try:\n            delta_fingerprint = self.fingerprinter.fingerprint(task)\n        except Exception:\n            delta_fingerprint = None \n\n        # 4. Create the \"Task Passport\"\n        profile = TaskProfile(\n            task_id=task_id,\n            difficulty_tier=tier,\n            basin=basin,\n            delta_fingerprint=delta_fingerprint,\n            difficulty_score=difficulty_score # <-- *** NEW (HOTFIX 11) ***\n        )\n        \n        profiler.end(f\"TaskAnalyzer.analyze.{task_id}\")\n        return profile\n\n    def _detect_basin(self, train_examples: List[Dict]) -> str:\n        \"\"\"Heuristically determines the 'type' of task (a fast, weak heuristic).\"\"\"\n        if not train_examples:\n            return 'unknown'\n        \n        features = {'has_shape_change': False, 'has_color_change': False, \n                    'has_object_count_change': False}\n        \n        for example in train_examples:\n            try:\n                inp, out = np.array(example['input']), np.array(example['output'])\n                if inp.shape != out.shape: \n                    features['has_shape_change'] = True\n                if not np.array_equal(inp, out):\n                    features['has_color_change'] = True\n            except Exception:\n                continue\n        \n        if features['has_shape_change']: return 'scaling'\n        if features['has_color_change']: return 'color_mapping'\n        return 'unknown'\n\nprint(\"  Defined: TaskAnalyzer (The Triage Engine)\")\nprint(\"=\"*70)\n#Cell 5\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:41:34.550502Z","iopub.execute_input":"2025-11-08T20:41:34.550842Z","iopub.status.idle":"2025-11-08T20:41:34.593859Z","shell.execute_reply.started":"2025-11-08T20:41:34.550811Z","shell.execute_reply":"2025-11-08T20:41:34.592676Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nðŸŒŠâš›ï¸ LucidOrca Solver: Cell 5 (LTM-v4) Fingerprinting & Triage [HOTFIX 11]\n  Defined: TaskProfile (The AGI's 'Task Passport')\n  Defined: VisionModelEncoder (Grid-to-Vector Engine)\n  Defined: TaskFingerprinter (The 'Probe' Generator)\n  Defined: TaskAnalyzer (The Triage Engine)\n======================================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#Cell 6\n################################################################################\n#\n# ðŸŒŠâš›ï¸ LUCIDORCA ULTIMATE SOLVER - (LTM-v4 REBUILD)\n#\n# Cell 6: The Cognitive Engine (CPU & Mind)\n#\n# *** LTM-v4 HOTFIX 19 (\"Hyper-Pruning\"): ***\n#\n# 1. Rationale: Our \"Slow Brain\" is still timing out. We will\n#    prune its action space even more aggressively to \"pay\" for\n#    the new `depth=20` search.\n# 2. `TOP_K_ACTIONS`: Decreased from 20 to 15.\n# 3. This forces the HPN-guided search to be even more \"focused\"\n#    and less \"creative,\" maximizing speed.\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(\"ðŸŒŠâš›ï¸ LucidOrca Solver: Cell 6 (LTM-v4 HOTFIX 19) The Cognitive Engine\")\n\n# --- 1. The \"CPU\" - Executes Abstract Programs (ASTs) ---\n\nclass SymbolicProgramInterpreter:\n    \"\"\"\n    The LTM-v4 \"CPU\" (Interpreter).\n    \"\"\"\n    \n    def __init__(self, perception_engine: PerceptionEngine):\n        # The \"ALU\" (Instruction Set) from Cell 4\n        self.primitives = MetaPrimitives(perception_engine)\n        \n        # This \"dispatch table\" maps 'op' strings from the AST\n        # to the actual Python functions in our MetaPrimitives class.\n        self.dispatch_table: Dict[str, Callable] = {\n            # --- Finder Ops ---\n            'find': self.primitives.find_objects,\n            'get_largest': self.primitives.get_largest,\n            'get_smallest': self.primitives.get_smallest,\n            \n            # --- Transformer Ops ---\n            'move': self.primitives.move,\n            'delete': self.primitives.delete,\n            'recolor': self.primitives.recolor,\n            'copy': self.primitives.copy_objects,\n            \n            # --- HOTFIX 12 Primitive ---\n            'draw_path': self.primitives.draw_path,\n            \n            # --- Procedural Op (Solves Blind Spot #5) ---\n            'map': self._op_map \n        }\n        print(\"  SymbolicProgramInterpreter (LTM-v4 'CPU') initialized. [HOTFIX 19]\")\n\n    def run_instruction(self, ctx: Dict, grid: np.ndarray, \n                        instruction: Dict) -> Tuple[Dict, np.ndarray]:\n        op_name = instruction.get('op')\n        params = instruction.get('params', {})\n        \n        if op_name not in self.dispatch_table:\n            raise ValueError(f\"Unknown operation: {op_name}\")\n            \n        op_function = self.dispatch_table[op_name]\n        \n        try:\n            new_ctx, new_grid = op_function(copy.deepcopy(ctx), grid, **params)\n            return new_ctx, new_grid\n        except Exception as e:\n            raise e\n            \n    def run(self, program_ast: List[Dict], initial_grid: np.ndarray) -> np.ndarray:\n        profiler.start(\"Interpreter.run\")\n        ctx = {'last_result': None}\n        current_grid = initial_grid.copy()\n        try:\n            for instruction in program_ast:\n                (ctx, current_grid) = self.run_instruction(ctx, current_grid, instruction)\n            profiler.end(\"Interpreter.run\")\n            return current_grid\n        except Exception as e:\n            profiler.end(\"Interpreter.run\")\n            return initial_grid\n\n    def _op_map(self, ctx: Dict, grid: np.ndarray, **params) -> Tuple[Dict, np.ndarray]:\n        target_key = params.get('target', 'last_result')\n        sub_program = params.get('program', []) \n        \n        if not sub_program: return ctx, grid \n        items_to_map = ctx.get(target_key, [])\n        if not items_to_map or not isinstance(items_to_map, list): return ctx, grid \n        \n        map_results = []\n        current_grid = grid \n        for item in list(items_to_map):\n            local_ctx = {'this': item, 'last_result': item}\n            try:\n                for instruction in sub_program:\n                    (local_ctx, current_grid) = self.run_instruction(\n                        local_ctx, current_grid, instruction\n                    )\n                map_results.append(local_ctx.get('last_result'))\n            except Exception:\n                continue\n        ctx['last_result'] = map_results\n        return ctx, current_grid\n\nprint(\"  Defined: SymbolicProgramInterpreter (The 'CPU') [HOTFIX 19]\")\n\n\n# --- 2. The \"Mind\" - Generates Abstract Programs (ASTs) ---\n\n@dataclass\nclass BeamEntry:\n    \"\"\"A single state in our Beam Search.\"\"\"\n    program_ast: List[Dict]\n    last_ctx: Dict\n    current_grid: np.ndarray\n    cost: float\n    \n    def __hash__(self):\n        return hash(self.current_grid.tobytes())\n    def __eq__(self, other):\n        return self.current_grid.tobytes() == other.current_grid.tobytes()\n\nclass SymbolicProgramSynthesizer:\n    \"\"\"\n    The LTM-v4 \"Mind\" (Solver).\n    \n    *** HOTFIX 19: Implements Hyper-Pruning. ***\n    \"\"\"\n    \n    # --- *** NEW (HOTFIX 19) *** ---\n    # We are pruning *even more* to \"pay\" for depth=20.\n    TOP_K_ACTIONS: int = 15 # Was 20\n    \n    def __init__(self, interpreter: SymbolicProgramInterpreter, \n                 cfg: ChampionshipConfig, \n                 fingerprinter: TaskFingerprinter,\n                 hpn_playbook_vectors: Optional[np.ndarray],\n                 hpn_playbook_programs: List,\n                 hpn_grammar: Dict):\n        \n        self.interpreter = interpreter\n        self.config = cfg\n        \n        # --- *** (HOTFIX 17) *** ---\n        self.fingerprinter = fingerprinter\n        self.hpn_playbook_vectors = hpn_playbook_vectors # The \"quiz\" fingerprints\n        self.hpn_playbook_programs = hpn_playbook_programs # The \"seed\" architectures\n        self.hpn_grammar = hpn_grammar # The \"transition matrix\"\n        \n        if self.hpn_playbook_vectors is not None and self.hpn_grammar:\n            print(\"  SymbolicProgramSynthesizer (LTM-v4 'Slow Brain') initialized. [HOTFIX 19 - HPN LOADED]\")\n        else:\n            print(\"  SymbolicProgramSynthesizer (LTM-v4 'Slow Brain') initialized. [HOTFIX 19 - HPN *NOT* LOADED]\")\n\n    def _heuristic(self, grid: np.ndarray, target_grid: np.ndarray) -> float:\n        \"\"\"The \"cost\" function for our search.\"\"\"\n        if grid.shape != target_grid.shape:\n            return 1000.0 + abs(grid.size - target_grid.size)\n        return np.sum(grid != target_grid)\n\n    def _precompute_finders(self, grid: np.ndarray, base_ctx: Dict = None) -> Dict:\n        \"\"\"\n        (From HOTFIX 7)\n        Runs a static, PRUNED set of \"Finder\" ops to populate the context.\n        \"\"\"\n        profiler.start(\"Synthesizer._precompute_finders\")\n        \n        if base_ctx:\n            ctx = copy.deepcopy(base_ctx)\n        else:\n            ctx = {'last_result': None}\n\n        all_objects = []\n        try:\n            (find_ctx, _) = self.interpreter.run_instruction(\n                ctx, grid, {'op': 'find', 'params': {}}\n            )\n            all_objects = find_ctx.get('last_result', [])\n            ctx['all_objects'] = all_objects\n        except Exception:\n            ctx['all_objects'] = []\n        \n        find_ctx_for_derivation = {'last_result': all_objects}\n        try:\n            (largest_ctx, _) = self.interpreter.run_instruction(\n                find_ctx_for_derivation, grid, \n                {'op': 'get_largest', 'params': {'target': 'last_result'}}\n            )\n            ctx['largest'] = largest_ctx.get('last_result', [])\n        except Exception:\n            ctx['largest'] = []\n\n        try:\n            (smallest_ctx, _) = self.interpreter.run_instruction(\n                find_ctx_for_derivation, grid, \n                {'op': 'get_smallest', 'params': {'target': 'last_result'}}\n            )\n            ctx['smallest'] = smallest_ctx.get('last_result', [])\n        except Exception:\n            ctx['smallest'] = []\n\n        profiler.end(\"Synthesizer._precompute_finders\")\n        return ctx\n\n    def _get_possible_ops(self, ctx: Dict) -> List[Dict]:\n        \"\"\"\n        (Unchanged from HOTFIX 17)\n        Generates the \"full-power\" action space.\n        \"\"\"\n        profiler.start(\"Synthesizer._get_possible_ops\")\n        possible_ops = []\n        \n        target_keys = [\n            k for k, v in ctx.items() \n            if isinstance(v, list) and k != 'last_result' and v\n        ]\n        \n        if not target_keys:\n            profiler.end(\"Synthesizer._get_possible_ops\")\n            return []\n            \n        deltas = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n        \n        for target_key in target_keys:\n            # --- 1. Simple \"Global\" Transformer Ops (Always On) ---\n            for color in range(10):\n                possible_ops.append(\n                    {'op': 'recolor', 'params': {'target': target_key, 'color': color}}\n                )\n            \n            for d in deltas:\n                possible_ops.append({'op': 'move', 'params': {'target': target_key, 'delta': d}})\n                possible_ops.append({'op': 'copy', 'params': {'target': target_key, 'delta': d}})\n                \n            possible_ops.append({'op': 'delete', 'params': {'target': target_key}})\n            \n            # --- 2. Drawing Primitives (HOTFIX 12) ---\n            for color in range(1, 10): # Iterate 1-9 (non-background)\n                possible_ops.append(\n                    {'op': 'draw_path', 'params': {'target': target_key, 'color': color}}\n                )\n        \n            # --- 3. Procedural Ops (ForEach loop) ---\n            map_sub_programs = []\n            map_target = 'this'\n            \n            for color in range(10):\n                map_sub_programs.append(\n                    [{'op': 'recolor', 'params': {'target': map_target, 'color': color}}]\n                )\n            map_sub_programs.append([{'op': 'delete', 'params': {'target': map_target}}])\n            \n            for d in deltas:\n                map_sub_programs.append([{'op': 'move', 'params': {'target': map_target, 'delta': d}}])\n                map_sub_programs.append([{'op': 'copy', 'params': {'target': map_target, 'delta': d}}])\n            \n            for sub_prog in map_sub_programs:\n                possible_ops.append({\n                    'op': 'map',\n                    'params': {\n                        'target': target_key,\n                        'program': sub_prog\n                    }\n                })\n        \n        profiler.end(\"Synthesizer._get_possible_ops\")\n        return possible_ops\n\n    def _get_playbook_seeds(self, task: Dict) -> List[BeamEntry]:\n        \"\"\"\n        (Unchanged from HOTFIX 17)\n        Phase A (\"Diagnose & Seed\")\n        \"\"\"\n        profiler.start(\"Synthesizer.get_playbook_seeds\")\n        seed_entries = []\n        \n        # 1. Diagnose: Fingerprint the new task\n        try:\n            task_fingerprint = self.fingerprinter.fingerprint(task)\n        except Exception:\n            profiler.end(\"Synthesizer.get_playbook_seeds\")\n            return [] \n\n        # 2. k-NN Lookup: Find closest \"quiz\" tasks\n        if (self.hpn_playbook_vectors is None or \n            len(self.hpn_playbook_programs) == 0):\n            profiler.end(\"Synthesizer.get_playbook_seeds\")\n            return []\n\n        try:\n            distances = np.linalg.norm(self.hpn_playbook_vectors - task_fingerprint, axis=1)\n            k = 3 \n            nearest_indices = np.argsort(distances)[:k]\n        except Exception:\n            profiler.end(\"Synthesizer.get_playbook_seeds\")\n            return []\n\n        # 3. Load Architectures: Build new BeamEntry objects from these seeds\n        initial_grid = np.array(task['train'][0]['input'])\n        target_grid = np.array(task['train'][0]['output'])\n        \n        for i in nearest_indices:\n            seed_program = self.hpn_playbook_programs[i]\n            \n            try:\n                seed_ctx = {'last_result': None}\n                seed_grid = initial_grid.copy()\n                \n                for instruction in seed_program:\n                    (seed_ctx, seed_grid) = self.interpreter.run_instruction(\n                        seed_ctx, seed_grid, instruction\n                    )\n                \n                final_seed_ctx = self._precompute_finders(seed_grid, base_ctx=seed_ctx)\n                seed_cost = self._heuristic(seed_grid, target_grid)\n\n                seed_entries.append(BeamEntry(\n                    program_ast=seed_program,\n                    last_ctx=final_seed_ctx,\n                    current_grid=seed_grid,\n                    cost=seed_cost\n                ))\n            except Exception:\n                continue\n\n        profiler.end(\"Synthesizer.get_playbook_seeds\")\n        return seed_entries\n\n    def solve(self, task: Dict, timeout: float) -> Tuple[Optional[List[Dict]], str]:\n        \"\"\"\n        *** HOTFIX 19: Implements \"Hyper-Pruning\" (Top-K=15). ***\n        \"\"\"\n        profiler.start(\"SymbolicProgramSynthesizer.solve\")\n        task_start_time = time.time()\n        \n        examples = task.get('train', [])\n        if not examples:\n            profiler.end(\"SymbolicProgramSynthesizer.solve\")\n            return None, \"Synthesizer.NoTrainData\"\n\n        try:\n            initial_grid = np.array(examples[0]['input'])\n            target_grid = np.array(examples[0]['output'])\n        except Exception:\n            profiler.end(\"SymbolicProgramSynthesizer.solve\")\n            return None, \"Synthesizer.BadData\"\n            \n        initial_cost = self._heuristic(initial_grid, target_grid)\n        if initial_cost == 0:\n            return [], \"Synthesizer.Identity\"\n        \n        \n        # --- Phase A (\"Playbook Seeding\") ---\n        beam = self._get_playbook_seeds(task)\n        \n        initial_ctx = self._precompute_finders(initial_grid)\n        beam.append(BeamEntry(\n            program_ast=[],\n            last_ctx=initial_ctx,\n            current_grid=initial_grid,\n            cost=initial_cost\n        ))\n        \n        visited_states: Dict[bytes, float] = {\n            entry.current_grid.tobytes(): entry.cost for entry in beam\n        }\n\n        # --- Phase B (\"Grammar-Guided & Pruned Search\") ---\n        \n        for depth in range(self.config.MAX_PROGRAM_DEPTH):\n            if (time.time() - task_start_time) > timeout:\n                profiler.end(\"SymbolicProgramSynthesizer.solve\")\n                return None, \"Synthesizer.Timeout.Coarse\"\n                \n            new_beam = []\n            \n            for entry_idx, entry in enumerate(beam):\n                \n                if len(entry.program_ast) >= self.config.MAX_PROGRAM_DEPTH:\n                    continue\n                \n                if entry_idx % (max(1, self.config.BEAM_SEARCH_WIDTH // 10)) == 0:\n                     if (time.time() - task_start_time) > timeout:\n                        profiler.end(\"SymbolicProgramSynthesizer.solve\")\n                        return None, \"Synthesizer.Timeout.Granular\"\n                \n                # --- HPN Guidance (Level 2: \"Grammar\") ---\n                \n                possible_ops = self._get_possible_ops(entry.last_ctx)\n                \n                if not entry.program_ast:\n                    last_op_name = \"START\"\n                else:\n                    last_op_name = entry.program_ast[-1].get('op', 'unknown')\n                \n                hpn_probabilities = self.hpn_grammar.get(last_op_name, {})\n                \n                prioritized_ops = sorted(\n                    possible_ops,\n                    key=lambda op: hpn_probabilities.get(op.get('op'), 0.0),\n                    reverse=True\n                )\n                \n                # --- *** NEW (HOTFIX 19): HYPER-PRUNING *** ---\n                ops_to_evaluate = prioritized_ops[:self.TOP_K_ACTIONS]\n                \n                \n                for op_ast in ops_to_evaluate: # <-- Iterate over the *pruned* list\n                    try:\n                        new_program = entry.program_ast + [op_ast]\n                        \n                        (temp_ctx, new_grid) = self.interpreter.run_instruction(\n                            entry.last_ctx, \n                            entry.current_grid, \n                            op_ast\n                        )\n                        \n                        new_ctx = self._precompute_finders(new_grid, base_ctx=temp_ctx)\n\n                        new_grid_hash = new_grid.tobytes()\n                        new_cost = self._heuristic(new_grid, target_grid)\n                        \n                        if visited_states.get(new_grid_hash, float('inf')) <= new_cost:\n                            continue\n                        visited_states[new_grid_hash] = new_cost\n\n                        if new_cost == 0.0:\n                            if self._validate_program(new_program, examples):\n                                profiler.end(\"SymbolicProgramSynthesizer.solve\")\n                                return new_program, f\"Synthesizer.Success.d{len(new_program)}\"\n                        \n                        new_beam.append(BeamEntry(new_program, new_ctx, new_grid, new_cost))\n                    \n                    except Exception:\n                        continue \n            \n            if not new_beam:\n                break \n            \n            combined_beam = new_beam + beam\n            combined_beam.sort(key=lambda e: e.cost)\n            \n            seen_grids = set()\n            unique_beam = []\n            for entry in combined_beam:\n                grid_hash = entry.current_grid.tobytes()\n                if grid_hash not in seen_grids:\n                    unique_beam.append(entry)\n                    seen_grids.add(grid_hash)\n            \n            beam = unique_beam[:self.config.BEAM_SEARCH_WIDTH]\n\n        profiler.end(\"SymbolicProgramSynthesizer.solve\")\n        return None, \"Synthesizer.Fail.MaxDepth\"\n\n    def _validate_program(self, program_ast: List[Dict], examples: List[Dict]) -> bool:\n        \"\"\"\n        Validates a candidate program against ALL training examples.\n        \"\"\"\n        for ex in examples:\n            try:\n                inp_grid = np.array(ex['input'])\n                out_grid = np.array(ex['output'])\n                \n                predicted_grid = self.interpreter.run(program_ast, inp_grid)\n                \n                if not np.array_equal(predicted_grid, out_grid):\n                    return False\n            except Exception:\n                return False \n                \n        return True\n\nprint(\"  Defined: SymbolicProgramSynthesizer (The 'Mind') [HOTFIX 19]\")\nprint(\"=\"*70)\n#Cell 6\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:41:34.595168Z","iopub.execute_input":"2025-11-08T20:41:34.595486Z","iopub.status.idle":"2025-11-08T20:41:34.643990Z","shell.execute_reply.started":"2025-11-08T20:41:34.595457Z","shell.execute_reply":"2025-11-08T20:41:34.642784Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nðŸŒŠâš›ï¸ LucidOrca Solver: Cell 6 (LTM-v4 HOTFIX 19) The Cognitive Engine\n  Defined: SymbolicProgramInterpreter (The 'CPU') [HOTFIX 19]\n  Defined: SymbolicProgramSynthesizer (The 'Mind') [HOTFIX 19]\n======================================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#Cell 7\n################################################################################\n#\n# ðŸŒŠâš›ï¸ LUCIDORCA ULTIMATE SOLVER - (LTM-v4 REBUILD)\n#\n# Cell 7: The Master Toolbox (Orchestrator & Assembly)\n#\n# *** LTM-v4 HOTFIX 17 (The *True* \"Two-Brain\" Assembly): ***\n#\n# 1. This is the synthesis of HOTFIX 15 and 16.\n# 2. `LucidOrcaUltimateSolver.__init__`: Now accepts *all* HPN\n#    components (playbook, grammar, etc.) from Cell 9.\n# 3. `HeuristicPlaybookSolver` (\"Fast Brain\"): Is instantiated and\n#    receives the \"Playbook\" HPN (L1).\n# 4. `SymbolicProgramSynthesizer` (\"Slow Brain\"): Is now *also*\n#    instantiated with *all* HPN components. It is now a\n#    \"schooled\" searcher, not a \"dumb\" one.\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(\"ðŸŒŠâš›ï¸ LucidOrca Solver: Cell 7 (LTM-v4) Master Toolbox & Assembly [HOTFIX 17]\")\n\n# --- 1. Consensus Utility (for Blind Spot #10) ---\n\nclass SolverAgreementEnsemble:\n    \"\"\"\n    Measures solver agreement. This is a utility for our final step\n    to \"judge\" the *results* (grids) from the Abstraction (LTM-v4 Query)\n    and Reasoning (LTM-v4 Search) passes.\n    \"\"\"\n    def __init__(self):\n        self.agreement_history = []\n        print(\"  SolverAgreementEnsemble (Consensus Utility) initialized.\")\n\n    def measure_agreement(self, grids: List[np.ndarray]) -> Tuple[float, np.ndarray]:\n        if not grids:\n            return 0.0, np.array([[0]])\n\n        grid_hashes = []\n        valid_grids = []\n        \n        for grid in grids:\n            if grid is not None and grid.size > 0:\n                grid_hashes.append(grid.tobytes())\n                valid_grids.append(grid)\n            else:\n                grid_hashes.append(\"NONE\")\n        \n        if not valid_grids:\n             return 0.0, np.array([[0]])\n\n        counts = Counter(grid_hashes)\n        most_common_bytes, most_common_count = counts.most_common(1)[0]\n        \n        agreement_ratio = most_common_count / len(grid_hashes)\n\n        if most_common_bytes != \"NONE\":\n            collapsed_solution = next(g for g in valid_grids if g.tobytes() == most_common_bytes)\n        else:\n            if len(valid_grids) > 0:\n                collapsed_solution = valid_grids[0]\n            else:\n                collapsed_solution = np.array([[0]])\n\n        self.agreement_history.append(agreement_ratio)\n        return agreement_ratio, collapsed_solution\n\nprint(\"  Defined: SolverAgreementEnsemble (Consensus Utility)\")\n\n\n# --- \"Brain 1\" (The \"Fast\" Architect - HOTFIX 16) ---\n\nclass HeuristicPlaybookSolver:\n    \"\"\"\n    This is our \"fast\" brain (\"Brain 1\"). It is the \"schooled\"\n    architect. It does *not* do a beam search. It only\n    checks its \"playbook\" of known-good architectures.\n    \"\"\"\n    def __init__(self, fingerprinter: TaskFingerprinter, \n                 interpreter: SymbolicProgramInterpreter,\n                 hpn_playbook_vectors: Optional[np.ndarray],\n                 hpn_playbook_programs: List):\n        \n        self.fingerprinter = fingerprinter\n        self.interpreter = interpreter\n        self.hpn_playbook_vectors = hpn_playbook_vectors\n        self.hpn_playbook_programs = hpn_playbook_programs\n        \n        if self.hpn_playbook_vectors is not None:\n            print(\"  HeuristicPlaybookSolver (LTM-v4 'Fast Brain') initialized. [HPN LOADED]\")\n        else:\n            print(\"  HeuristicPlaybookSolver (LTM-v4 'Fast Brain') initialized. [HPN *NOT* LOADED]\")\n\n    def _validate_program(self, program_ast: List[Dict], examples: List[Dict]) -> bool:\n        \"\"\"\n        Validates a candidate program against ALL training examples.\n        \"\"\"\n        for ex in examples:\n            try:\n                inp_grid = np.array(ex['input'])\n                out_grid = np.array(ex['output'])\n                predicted_grid = self.interpreter.run(program_ast, inp_grid)\n                if not np.array_equal(predicted_grid, out_grid):\n                    return False\n            except Exception:\n                return False \n        return True\n\n    def solve(self, task: Dict, timeout: float) -> Tuple[Optional[List[Dict]], str]:\n        \"\"\"\n        This is the \"fast-pass\" cognitive loop:\n        1. Diagnose (fingerprint)\n        2. k-NN Lookup (check HPN Playbook)\n        3. Validate (run the 5 best-guess architectures)\n        \"\"\"\n        profiler.start(\"HeuristicPlaybookSolver.solve\")\n        \n        examples = task.get('train', [])\n        if not examples:\n            profiler.end(\"HeuristicPlaybookSolver.solve\")\n            return None, \"Playbook.NoTrainData\"\n            \n        # 1. Diagnose: Fingerprint the new task\n        try:\n            task_fingerprint = self.fingerprinter.fingerprint(task)\n        except Exception:\n            profiler.end(\"HeuristicPlaybookSolver.solve\")\n            return None, \"Playbook.Fail.Fingerprint\"\n\n        # 2. k-NN Lookup: Find 5 closest \"quiz\" architectures\n        if (self.hpn_playbook_vectors is None or \n            len(self.hpn_playbook_programs) == 0):\n            profiler.end(\"HeuristicPlaybookSolver.solve\")\n            return None, \"Playbook.Fail.NoHPN\"\n\n        try:\n            distances = np.linalg.norm(self.hpn_playbook_vectors - task_fingerprint, axis=1)\n            k = 5 # Check the top 5 \"plays\"\n            nearest_indices = np.argsort(distances)[:k]\n        except Exception:\n            profiler.end(\"HeuristicPlaybookSolver.solve\")\n            return None, \"Playbook.Fail.KNN\"\n\n        # 3. Validate: Run the \"Playbook\"\n        for i in nearest_indices:\n            seed_program = self.hpn_playbook_programs[i]\n            \n            # Check if this \"seed\" program *as-is* solves the task\n            if self._validate_program(seed_program, examples):\n                profiler.end(\"HeuristicPlaybookSolver.solve\")\n                return seed_program, \"Playbook.Success\"\n        \n        # If no \"play\" worked...\n        profiler.end(\"HeuristicPlaybookSolver.solve\")\n        return None, \"Playbook.Fail.NoMatch\"\n\nprint(\"  Defined: HeuristicPlaybookSolver (The 'Fast Brain') [HOTFIX 17]\")\n\n\n# --- 2. The LTM-v4 Master \"Toolbox\" Class (Assembly) ---\n\nclass LucidOrcaUltimateSolver:\n    \"\"\"\n    The main \"Factory Manager\" or \"Toolbox\" class for LTM-v4.\n    \n    *** HOTFIX 17: Now assembles the *true* \"Two-Brain\" model. ***\n    \"\"\"\n    def __init__(self, cfg: ChampionshipConfig, \n                 hpn_playbook_vectors: Optional[np.ndarray],\n                 hpn_playbook_programs: List,\n                 hpn_grammar: Dict): # <-- *** CORRECTED (HOTFIX 17) ***\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"ðŸŒŠâš›ï¸ Initializing LucidOrcaUltimateSolver (LTM-v4 Master Toolbox)...\")\n        \n        self.config = cfg\n        \n        # --- 1. Instantiate Core \"Vision\" (from Cell 3) ---\n        self.perception_engine = PerceptionEngine()\n        print(f\"  âœ… 1. PerceptionEngine (Vision)... LOADED\")\n\n        # --- 2. Instantiate \"Triage & Fingerprinting\" (from Cell 5) ---\n        self.analyzer = TaskAnalyzer(self.perception_engine)\n        self.fingerprinter = self.analyzer.fingerprinter\n        print(f\"  âœ… 2. TaskAnalyzer (Triage & Fingerprinting)... LOADED\")\n\n        # --- 3. Instantiate The \"CPU\" (from Cell 6) ---\n        self.interpreter = SymbolicProgramInterpreter(self.perception_engine)\n        print(f\"  âœ… 3. SymbolicProgramInterpreter ('CPU')... LOADED\")\n\n        # --- 4. Instantiate The \"Fast Brain\" (Brain 1) ---\n        self.heuristic_solver = HeuristicPlaybookSolver(\n            self.fingerprinter,\n            self.interpreter,\n            hpn_playbook_vectors,\n            hpn_playbook_programs\n        )\n        print(f\"  âœ… 4. HeuristicPlaybookSolver ('Fast Brain')... LOADED\")\n\n        # --- 5. Instantiate The \"Schooled Slow Brain\" (Brain 2) ---\n        # *** CORRECTED (HOTFIX 17): We now pass *all* HPN components to\n        # the \"slow\" brain, making it a \"schooled\" searcher.\n        self.synthesizer = SymbolicProgramSynthesizer(\n            self.interpreter, \n            self.config, \n            self.fingerprinter,\n            hpn_playbook_vectors,\n            hpn_playbook_programs,\n            hpn_grammar\n        )\n        print(f\"  âœ… 5. SymbolicProgramSynthesizer ('Schooled Slow Brain')... LOADED\")\n        \n        # --- 6. Instantiate Utilities ---\n        self.utilities = {\n            'agreement_ensemble': SolverAgreementEnsemble()\n        }\n        print(\"  âœ… 6. Consensus & Utility Modules... LOADED\")\n\n        print(\"\\nâœ… LucidOrca LTM-v4 Solver (Toolbox) is ready!\"); print(\"=\"*70)\n\nprint(\"=\"*70)\n#Cell 7\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:41:34.644970Z","iopub.execute_input":"2025-11-08T20:41:34.645207Z","iopub.status.idle":"2025-11-08T20:41:34.673210Z","shell.execute_reply.started":"2025-11-08T20:41:34.645189Z","shell.execute_reply":"2025-11-08T20:41:34.672041Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nðŸŒŠâš›ï¸ LucidOrca Solver: Cell 7 (LTM-v4) Master Toolbox & Assembly [HOTFIX 17]\n  Defined: SolverAgreementEnsemble (Consensus Utility)\n  Defined: HeuristicPlaybookSolver (The 'Fast Brain') [HOTFIX 17]\n======================================================================\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#Cell 8\n################################################################################\n#\n# ðŸŒŠâš›ï¸ LUCIDORCA ULTIMATE SOLVER - (LTM-v4 REBUILD)\n#\n# Cell 8: Load All Task Data\n#\n# This cell defines the data loading functions and loads all necessary\n# JSON files from the Kaggle environment.\n#\n# 1. `test_tasks`: The set of tasks we must solve for submission.\n# 2. `training_tasks` & `evaluation_tasks`: The 1120 known tasks\n#    that our \"Game Genie\" (Cell 9) will use to \"train\" (build its LTM).\n# 3. `training_solutions` & `evaluation_solutions`: The ground-truth\n#    solutions required by Cell 9 to *validate* the rules it finds.\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(\"ðŸŒŠâš›ï¸ LucidOrca Solver: Cell 8 (LTM-v4) Load Task Data\")\n\n# --- 1. Define Data Paths ---\\n# This assumes the standard Kaggle competition dataset path\nDATA_DIR = Path(\"/kaggle/input/arc-prize-2025\")\n\n# The \"unknown\" tasks for our final submission\nTEST_CHALLENGES_PATH = DATA_DIR / \"arc-agi_test_challenges.json\"\n\n# The \"known\" tasks for our Game Genie (LTM)\nTRAINING_CHALLENGES_PATH = DATA_DIR / \"arc-agi_training_challenges.json\"\nEVALUATION_CHALLENGES_PATH = DATA_DIR / \"arc-agi_evaluation_challenges.json\"\n\n# The \"ground truth\" for our Game Genie (LTM)\nTRAINING_SOLUTIONS_PATH = DATA_DIR / \"arc-agi_training_solutions.json\"\nEVALUATION_SOLUTIONS_PATH = DATA_DIR / \"arc-agi_evaluation_solutions.json\"\n\n\n# --- 2. Data Loading Function ---\\n\ndef load_json_tasks(file_path: Path) -> Dict[str, Dict]:\n    \"\"\"\n    Loads a JSON task file from the given path.\n    Returns a dictionary of {task_id: task_data}.\n    \"\"\"\n    if not file_path.exists():\n        print(f\"  âš ï¸  WARNING: Task file not found at: {file_path}\")\n        print(f\"     Please ensure the 'ARC Prize 2025' dataset is added to this notebook.\")\n        return {}\n    \n    try:\n        profiler.start(f\"load_json.{file_path.name}\")\n        with open(file_path, 'r') as f:\n            tasks = json.load(f)\n        profiler.end(f\"load_json.{file_path.name}\")\n        \n        print(f\"  âœ… Loaded {len(tasks)} tasks from {file_path.name}\")\n        return tasks\n        \n    except Exception as e:\n        print(f\"  âŒ CRITICAL ERROR: Could not load or parse {file_path.name}: {e}\")\n        return {}\n\n# --- 3. Load the Test Set (Primary Goal) ---\\nprint(\"\\nLoading test set for submission...\")\ntest_tasks = load_json_tasks(TEST_CHALLENGES_PATH)\n\nif test_tasks:\n    print(f\"\\nSample task IDs from test set:\")\n    for i, task_id in enumerate(list(test_tasks.keys())[:3]):\n        try:\n            task = test_tasks[task_id]\n            print(f\"   {i+1}. {task_id}: {len(task.get('test', []))} test cases\")\n        except Exception as e:\n            print(f\"   {i+1}. {task_id}: Error parsing task info - {e}\")\nelse:\n    print(\"  No test tasks loaded. Submission will not be possible.\")\n\n# --- 4. Load Training/Evaluation Data (for \"Game Genie\" in Cell 9) ---\nprint(\"\\nLoading supplemental data for 'Game Genie' LTM pre-computation...\")\ntraining_tasks = load_json_tasks(TRAINING_CHALLENGES_PATH)\ntraining_solutions = load_json_tasks(TRAINING_SOLUTIONS_PATH)\nevaluation_tasks = load_json_tasks(EVALUATION_CHALLENGES_PATH)\nevaluation_solutions = load_json_tasks(EVALUATION_SOLUTIONS_PATH)\n\nprint(\"=\"*70)\n#Cell 8\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:41:34.674197Z","iopub.execute_input":"2025-11-08T20:41:34.674504Z","iopub.status.idle":"2025-11-08T20:41:35.183817Z","shell.execute_reply.started":"2025-11-08T20:41:34.674479Z","shell.execute_reply":"2025-11-08T20:41:35.182912Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nðŸŒŠâš›ï¸ LucidOrca Solver: Cell 8 (LTM-v4) Load Task Data\n  âœ… Loaded 240 tasks from arc-agi_test_challenges.json\n\nSample task IDs from test set:\n   1. 00576224: 1 test cases\n   2. 007bbfb7: 1 test cases\n   3. 009d5c81: 1 test cases\n\nLoading supplemental data for 'Game Genie' LTM pre-computation...\n  âœ… Loaded 1000 tasks from arc-agi_training_challenges.json\n  âœ… Loaded 1000 tasks from arc-agi_training_solutions.json\n  âœ… Loaded 120 tasks from arc-agi_evaluation_challenges.json\n  âœ… Loaded 120 tasks from arc-agi_evaluation_solutions.json\n======================================================================\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#Cell 9\n################################################################################\n#\n# ðŸŒŠâš›ï¸ LUCIDORCA ULTIMATE SOLVER - (LTM-v4 REBUILD)\n#\n# Cell 9: RSC Controller & \"Game Genie\" LTM-v4 Trainer\n#\n# *** LTM-v4 HOTFIX 19 (The *True* \"Brute-Force Time\" Test): ***\n#\n# 1. Rationale: The HOTFIX 18 run proved our \"Fast Brain\" (Playbook)\n#    was \"cheating\" on the 10 \"Punt\" tasks, solving them in 0.01s\n#    and preventing the \"Schooled Slow Brain\" from ever running.\n# 2. `run_pre_computation`: Now passes `is_punt_task=True` to the\n#    solver loop for the 10 \"Punt\" tasks.\n# 3. `_find_ground_truth_program`:\n#    - Now accepts an `is_punt_task` flag.\n#    - **IF `is_punt_task` IS TRUE:** It **SKIPS** the \"Fast Brain\"\n#      and *forces* the \"Schooled Slow Brain\" (`SymbolicProgramSynthesizer`)\n#      to run with the full 60-second budget.\n#    - **ELSE:** It uses the normal \"Fast Brain -> Slow Brain\"\n#      fallback logic for the \"Standard\" tasks.\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(\"ðŸŒŠâš›ï¸ LucidOrca Solver: Cell 9 (LTM-v4) RSC Controller & LTM Trainer [HOTFIX 19]\")\n\n# --- 1. Define the RSC Controller (LTM-v4) ---\n\nclass RSC_Controller:\n    \"\"\"\n    Implements the Recursive Symbolic Coherence (RSC) Framework.\n    *** HOTFIX 19: Manages the \"True Two-Brain\" HPN architecture. ***\n    \"\"\"\n    \n    # --- LTM Training Budget (HOTFIX 16) ---\n    LTM_BUDGET_PERCENT: float = CONFIG.LTM_BUDGET_PERCENT\n    LTM_NUM_TASKS: float = 1120.0 # 1000 train + 120 eval\n    \n    # --- Log Headers (HOTFIX 9) ---\n    LTM_LOG_COLUMNS = [\n        \"phase\", \"task_id\", \"task_tier\", \"status\", \n        \"time_taken_s\", \"program_cached\"\n    ]\n    INFERENCE_LOG_COLUMNS = [\n        \"phase\", \"task_id\", \"task_tier\", \"basin\", \n        \"ltm_status\", \"reasoning_status\", \"final_program\", \"time_taken_s\"\n    ]\n    \n    def __init__(self, cfg: ChampionshipConfig):\n        print(\"\\nInitializing RSC_Controller (LTM-v4)...\")\n        self.config = cfg\n        \n        self.log_path = Path(\"/kaggle/working/lucid_metrics.csv\")\n        self.metric_logger = MetricLogger(self.log_path)\n        \n        # --- *** (HOTFIX 17.1) *** ---\n        # 1. Build a *temporary* \"Toolbox\" just to get the fingerprinter\n        print(\"  Bootstrapping Cognitive Architecture (Phase 1)...\")\n        bootcamp_toolbox = LucidOrcaUltimateSolver(cfg, None, [], {}) \n        \n        # 2. Run the bootcamp to build the HPN \"Playbook\"\n        print(\"  Bootstrapping Cognitive Architecture (Phase 2)...\")\n        profiler.start(\"GameGenie.Bootcamp\")\n        (\n            self.hpn_playbook_vectors,\n            self.hpn_playbook_programs,\n            self.hpn_grammar\n        ) = self._run_architectural_bootcamp(bootcamp_toolbox.fingerprinter)\n        profiler.end(\"GameGenie.Bootcamp\")\n        print(f\"  âœ… Cognitive Bootcamp complete. Built HPN Grammar ({len(self.hpn_grammar)} rules) \"\n              f\"and HPN Playbook ({len(self.hpn_playbook_programs)} seeds).\")\n        \n        # 3. Build the *real* \"Two-Brain\" Toolbox\n        self.solver_toolbox = LucidOrcaUltimateSolver(\n            cfg,\n            self.hpn_playbook_vectors,\n            self.hpn_playbook_programs,\n            self.hpn_grammar\n        )\n        \n        # --- LTM-v4: \"Game Genie\" Long-Term Memory ---\n        self._ltm_vectors_list: List[np.ndarray] = []\n        self.ltm_programs: List[List[Dict]] = [] \n        self.ltm_vectors: Optional[np.ndarray] = None \n        self.task_profiles: Dict[str, TaskProfile] = {}\n        self.time_allocations = {\n            'abstraction_per_task': {},\n            'reasoning_per_task': {},\n        }\n        print(\"  âœ… RSC_Controller is online (LTM-v4 Architecture).\")\n\n    def _run_architectural_bootcamp(self, fingerprinter: TaskFingerprinter\n                                   ) -> Tuple[Optional[np.ndarray], List, Dict]:\n        \"\"\"\n        *** (Unchanged from HOTFIX 17.1): This is the \"School\". ***\n        \n        Builds both HPNs (Level 1 Playbook, Level 2 Grammar).\n        \"\"\"\n        \n        # 1. The \"Curriculum\" (Core Architectures)\n        CURRICULUM = [\n            [{'op': 'find'}, {'op': 'recolor'}], [{'op': 'find'}, {'op': 'move'}],\n            [{'op': 'find'}, {'op': 'copy'}], [{'op': 'find'}, {'op': 'delete'}],\n            [{'op': 'get_largest'}, {'op': 'recolor'}], [{'op': 'get_smallest'}, {'op': 'move'}],\n            [{'op': 'get_largest'}, {'op': 'copy'}], [{'op': 'get_smallest'}, {'op': 'delete'}],\n            [{'op': 'find'}, {'op': 'draw_path'}], [{'op': 'get_largest'}, {'op': 'draw_path'}],\n            [{'op': 'find'}, {'op': 'map', 'program': [{'op': 'recolor'}]}],\n            [{'op': 'find'}, {'op': 'map', 'program': [{'op': 'move'}]}],\n            [{'op': 'find'}, {'op': 'map', 'program': [{'op': 'delete'}]}],\n            [{'op': 'copy'}, {'op': 'find'}, {'op': 'recolor'}],\n            [{'op': 'move'}, {'op': 'find'}, {'op': 'delete'}],\n            [{'op': 'draw_path'}, {'op': 'find'}, {'op': 'recolor'}],\n            [{'op': 'map'}, {'op': 'find'}, {'op': 'move'}], [{'op': 'find'}]\n        ]\n\n        # 2. The \"Pop Quiz\" (Atomic Tasks for Playbook)\n        QUIZ_TASKS = {\n            'recolor_all': {'task': {'train': [{'input': [[1]], 'output': [[2]]}]}, \n                            'arch': [{'op': 'find'}, {'op': 'recolor', 'params': {'color': 2}}]},\n            'move_all': {'task': {'train': [{'input': [[1,0]], 'output': [[0,1]]}]},\n                         'arch': [{'op': 'find'}, {'op': 'move', 'params': {'delta': (0,1)}}]},\n            'copy_all': {'task': {'train': [{'input': [[1,0]], 'output': [[1,1]]}]},\n                         'arch': [{'op': 'find'}, {'op': 'copy', 'params': {'delta': (0,1)}}]},\n            'delete_all': {'task': {'train': [{'input': [[1]], 'output': [[0]]}]},\n                           'arch': [{'op': 'find'}, {'op': 'delete'}]},\n            'draw_path': {'task': {'train': [{'input': [[1,0,1]], 'output': [[1,5,1]]}]},\n                          'arch': [{'op': 'find'}, {'op': 'draw_path', 'params': {'color': 5}}]},\n            'map_delete': {'task': {'train': [{'input': [[1,2,1]], 'output': [[2]]}]}, # Deletes color 1\n                           'arch': [{'op': 'find', 'params': {'color': 1}}, \n                                    {'op': 'map', 'program': [{'op': 'delete'}]}]},\n            'move_largest': {'task': {'train': [{'input': [[1,0],[2,2,0]], 'output': [[1,0],[0,2,2]]}]},\n                             'arch': [{'op': 'get_largest'}, {'op': 'move', 'params': {'delta': (0,1)}}]},\n            'delete_smallest': {'task': {'train': [{'input': [[1,0],[2,2,0]], 'output': [[0,0],[2,2,0]]}]},\n                                'arch': [{'op': 'get_smallest'}, {'op': 'delete'}]},\n        }\n\n        # 3. Build HPN Level 1 (The \"Playbook\")\n        playbook_vectors = []\n        playbook_programs = []\n        \n        for name, data in QUIZ_TASKS.items():\n            try:\n                v = fingerprinter.fingerprint(data['task'])\n                playbook_vectors.append(v)\n                playbook_programs.append(data['arch'])\n            except Exception as e:\n                print(f\"  âš ï¸  Bootcamp HPN-L1 failed for {name}: {e}\")\n                continue\n        \n        final_playbook_vectors = None\n        if playbook_vectors:\n            final_playbook_vectors = np.stack(playbook_vectors)\n\n        # 4. Build HPN Level 2 (The \"Grammar\")\n        hpn_counts = defaultdict(lambda: defaultdict(int))\n        for program_ast in CURRICULUM:\n            prev_op = 'START'\n            for instruction in program_ast:\n                op_name = instruction['op']\n                hpn_counts[prev_op][op_name] += 1\n                prev_op = op_name\n        \n        hpn_grammar = defaultdict(dict)\n        for prev_op, next_ops in hpn_counts.items():\n            total_transitions = sum(next_ops.values())\n            if total_transitions > 0:\n                for next_op, count in next_ops.items():\n                    hpn_grammar[prev_op][next_op] = count / total_transitions\n        \n        return final_playbook_vectors, playbook_programs, hpn_grammar\n\n    def run_pre_computation(self, all_known_tasks, all_known_solutions):\n        \"\"\"\n        *** LTM-v4 \"GAME GENIE\" (On-the-fly Training) ***\n        *** HOTFIX 19: Passes `is_punt_task` flag. ***\n        \"\"\"\n        print(\"\\n--- ðŸ§  EXECUTING PRE-COMPUTATION: 'GAME GENIE' LTM-v4 TRAINING ---\")\n        if not all_known_tasks or not all_known_solutions:\n            print(\"  âš ï¸  Missing training/eval data. Skipping 'Game Genie' training.\")\n            return\n\n        profiler.start(\"GameGenie_LTMv4_Training\")\n        self.metric_logger.write_header(self.LTM_LOG_COLUMNS)\n        \n        fingerprinter = self.solver_toolbox.fingerprinter\n        total_tasks_to_train = len(all_known_tasks)\n        tasks_cached = 0\n        \n        # --- HOTFIX 16: Dynamic Budget Calculation ---\n        total_ltm_budget = self.config.total_time_budget * self.config.LTM_BUDGET_PERCENT\n        \n        if self.config.DIAGNOSTIC_RUN:\n             diag_scaled_budget = 0.0\n             if self.LTM_NUM_TASKS > 0:\n                 diag_scaled_budget = (total_ltm_budget / self.LTM_NUM_TASKS) * total_tasks_to_train\n             \n             min_budget_seconds = self.config.DIAGNOSTIC_MIN_RUNTIME_MINUTES * 60\n             total_ltm_budget = max(diag_scaled_budget, min_budget_seconds)\n             \n             print(f\"  *** âš ï¸  DIAGNOSTIC MODE: Training on {total_tasks_to_train} tasks... ***\")\n             print(f\"  Scaled LTM Budget: {total_ltm_budget / 60:.2f} minutes ({self.config.DIAGNOSTIC_MIN_RUNTIME_MINUTES:.0f}-min floor enforced)\")\n        else:\n             print(f\"  Running 'Game Genie' LTM training on {total_tasks_to_train} tasks.\")\n             print(f\"  Total LTM Budget: {total_ltm_budget / 3600:.2f} hours.\")\n        \n        \n        # --- HOTFIX 11: Phase 0 - Triage ALL tasks first ---\n        print(\"  Phase 0: Triaging all tasks for curriculum...\")\n        profiler.start(\"GameGenie.TriageAll\")\n        sorted_task_list = []\n        for task_id, task_data in all_known_tasks.items():\n            if task_id not in all_known_solutions:\n                continue\n            difficulty = estimate_task_difficulty(task_data)\n            tier = 'easy' if difficulty < 7.0 else ('medium' if difficulty < 18.0 else 'hard')\n            sorted_task_list.append((task_id, task_data, difficulty, tier))\n        \n        sorted_task_list.sort(key=lambda x: x[2])\n        profiler.end(\"GameGenie.TriageAll\")\n        \n        \n        # --- HOTFIX 16: \"Shaving\" Budget Calculation ---\n        num_punt_tasks = 10\n        num_standard_tasks = max(0, total_tasks_to_train - num_punt_tasks)\n        \n        punt_timeout = self.config.PUNT_TASK_BUDGET_SECONDS # 60.0s\n        total_punt_cost = num_punt_tasks * punt_timeout # 10 * 60s = 600s\n        \n        remaining_budget = total_ltm_budget - total_punt_cost\n        \n        if num_standard_tasks > 0:\n            standard_timeout = remaining_budget / num_standard_tasks\n        else:\n            standard_timeout = 0\n        \n        if standard_timeout < 1.0:\n            if remaining_budget > 0:\n                 print(f\"  âš ï¸  WARNING: Punt tasks consumed most of LTM budget. \"\n                       f\"Standard tasks will have {standard_timeout:.2f}s timeout.\")\n            standard_timeout = max(1.0, standard_timeout) # 1s minimum\n            \n        \n        tasks_to_punt = sorted_task_list[:num_punt_tasks]\n        tasks_to_standard_solve = sorted_task_list[num_punt_tasks:]\n\n        print(f\"  Budgeting: Standard Timeout = {standard_timeout:.2f}s | Punt Timeout = {punt_timeout:.2f}s\")\n        \n        \n        # --- HOTFIX 19: Phase A - \"Punt\" Tasks (FORCED SLOW BRAIN) ---\n        print(f\"\\n  --- Phase A: Solving {len(tasks_to_punt)} Easiest Tasks ({punt_timeout:.0f}s Budget) ---\")\n        tasks_processed = 0\n        try:\n            for (task_id, task_data, difficulty, tier) in tasks_to_punt:\n                task_start_time = time.time()\n                status = \"Fail.Unknown\"\n                program_cached = False\n                \n                try:\n                    ground_truth_outputs = all_known_solutions[task_id]\n                    (validated_program_ast, status) = self._find_ground_truth_program(\n                        task_id, task_data, ground_truth_outputs, \n                        timeout=punt_timeout, \n                        is_punt_task=True # <-- *** HOTFIX 19 FLAG ***\n                    )\n                    \n                    if validated_program_ast is not None:\n                        v_delta = fingerprinter.fingerprint(task_data)\n                        self._ltm_vectors_list.append(v_delta)\n                        self.ltm_programs.append(validated_program_ast)\n                        program_cached = True\n                        tasks_cached += 1\n                \n                except Exception as e:\n                    status = f\"Fail.Crash.{type(e).__name__}\"\n                finally:\n                    time_taken = time.time() - task_start_time\n                    self.metric_logger.log({\n                        'columns_order': self.LTM_LOG_COLUMNS, 'phase': \"GameGenie.Punt\",\n                        'task_id': task_id, 'task_tier': tier, 'status': status,\n                        'time_taken_s': f\"{time_taken:.3f}\", 'program_cached': program_cached\n                    })\n                    tasks_processed += 1\n                    print(f\"  Punt Task {tasks_processed}/{len(tasks_to_punt)}: {task_id} \"\n                          f\"({tier}) -> {status}. (Cached: {program_cached})\")\n\n        except Exception as e:\n            print(f\"  ðŸ”¥ðŸ”¥ðŸ”¥ CRASH during LTM PUNT phase: {e}\")\n\n\n        # --- HOTFIX 19: Phase B - \"Standard\" Tasks (Fast -> Slow Fallback) ---\n        print(f\"\\n  --- Phase B: Solving {len(tasks_to_standard_solve)} Remaining Tasks ({standard_timeout:.2f}s Budget) ---\")\n        try:\n            for (task_id, task_data, difficulty, tier) in tasks_to_standard_solve:\n                task_start_time = time.time()\n                status = \"Fail.Unknown\"\n                program_cached = False\n                \n                try:\n                    ground_truth_outputs = all_known_solutions[task_id]\n                    (validated_program_ast, status) = self._find_ground_truth_program(\n                        task_id, task_data, ground_truth_outputs, \n                        timeout=standard_timeout, \n                        is_punt_task=False # <-- *** HOTFIX 19 FLAG ***\n                    )\n                    \n                    if validated_program_ast is not None:\n                        v_delta = fingerprinter.fingerprint(task_data)\n                        self._ltm_vectors_list.append(v_delta)\n                        self.ltm_programs.append(validated_program_ast)\n                        program_cached = True\n                        tasks_cached += 1\n                \n                except Exception as e:\n                    status = f\"Fail.Crash.{type(e).__name__}\"\n                finally:\n                    time_taken = time.time() - task_start_time\n                    self.metric_logger.log({\n                        'columns_order': self.LTM_LOG_COLUMNS, 'phase': \"GameGenie.Standard\",\n                        'task_id': task_id, 'task_tier': tier, 'status': status,\n                        'time_taken_s': f\"{time_taken:.3f}\", 'program_cached': program_cached\n                    })\n                    tasks_processed += 1\n                    \n                    if (tasks_processed % 10 == 0) or (tasks_processed == total_tasks_to_train):\n                        print(f\"  LTM Training Progress... Task {tasks_processed + num_punt_tasks}/{total_tasks_to_train} processed. \"\n                              f\"({tasks_cached} programs cached)\")\n\n        except Exception as e:\n            print(f\"  ðŸ”¥ðŸ”¥ðŸ”¥ CRASH during LTM STANDARD phase: {e}\")\n            import traceback\n            traceback.print_exc()\n\n        # --- Finalize ---\n        if self._ltm_vectors_list:\n            self.ltm_vectors = np.stack(self._ltm_vectors_list)\n            print(f\"\\n  âœ… 'Game Genie' LTM-v4 Training complete.\")\n            print(f\"  Cached {self.ltm_vectors.shape[0]} programs in a \"\n                  f\"({self.ltm_vectors.shape[0]} x {self.ltm_vectors.shape[1]}) vector database.\")\n        else:\n            print(f\"\\n  âš ï¸  'Game Genie' LTM-v4 Training FAILED. No programs were cached.\")\n\n        profiler.end(\"GameGenie_LTMv4_Training\")\n\n    def _find_ground_truth_program(self, task_id: str, task_data: Dict, \n                                   ground_truth_outputs: List[Dict], timeout: float,\n                                   is_punt_task: bool = False # <-- *** HOTFIX 19 FLAG ***\n                                   ) -> Tuple[Optional[List[Dict]], str]:\n        \"\"\"\n        *** HOTFIX 19: Implements logic to *force* slow brain on punt tasks. ***\n        \"\"\"\n        profiler.start(f\"GameGenie.find_program.{task_id}\")\n        \n        program_ast = None\n        rule_name = \"Fail.Unknown\"\n\n        # --- *** HOTFIX 19: New Logic *** ---\n        # If it's *not* a punt task, we try the \"Fast Brain\" first.\n        # If it *is* a punt task, we skip this and go straight to the \"Slow Brain\".\n        if not is_punt_task:\n            # --- 1. Call \"Fast Brain\" (The \"Architect\") ---\n            fast_brain_timeout = min(timeout, 2.0) # Give 2s for a quick playbook check\n            fast_solver = self.solver_toolbox.heuristic_solver\n            program_ast, rule_name = fast_solver.solve(task_data, timeout=fast_brain_timeout)\n            \n            if program_ast is not None:\n                # We found a match, now just validate it\n                holdout_inputs = task_data.get('test', [])\n                if self._validate_on_holdout(program_ast, holdout_inputs, ground_truth_outputs):\n                    profiler.end(f\"GameGenie.find_program.{task_id}\")\n                    return program_ast, rule_name # e.g., \"Playbook.Success\"\n                else:\n                    rule_name = \"Playbook.Fail.Holdout\"\n                    program_ast = None # Clear it, force slow brain to run\n        \n        # --- 2. Call \"Schooled Slow Brain\" (The \"Searcher\") ---\n        # This now runs if:\n        # a) It's a \"Punt\" task (we skipped the fast brain).\n        # b) It's a \"Standard\" task and the fast brain failed.\n        if program_ast is None:\n            slow_solver = self.solver_toolbox.synthesizer\n            program_ast, rule_name = slow_solver.solve(task_data, timeout=timeout) \n        \n        if program_ast is None:\n            profiler.end(f\"GameGenie.find_program.{task_id}\")\n            return None, rule_name # e.g., \"Synthesizer.Fail.MaxDepth\"\n            \n        # --- 3. Final Validation ---\n        holdout_inputs = task_data.get('test', [])\n        \n        if self._validate_on_holdout(program_ast, holdout_inputs, ground_truth_outputs):\n            profiler.end(f\"GameGenie.find_program.{task_id}\")\n            return program_ast, rule_name # e.g., \"Synthesizer.Success.d{N}\"\n        else:\n            profiler.end(f\"GameGenie.find_program.{task_id}\")\n            return None, \"Synthesizer.Fail.Holdout\"\n\n    def _validate_on_holdout(self, program_ast: List[Dict], \n                               test_inputs: List[Dict], \n                               ground_truth_solutions: List[Dict]) -> bool:\n        \"\"\" (Unchanged from HOTFIX 4) \"\"\"\n        interpreter = self.solver_toolbox.interpreter\n        \n        if len(test_inputs) != len(ground_truth_solutions):\n            return False \n        if not test_inputs:\n            return True \n\n        try:\n            for i in range(len(test_inputs)):\n                inp_grid = np.array(test_inputs[i]['input'])\n                expected_output_grid = np.array(ground_truth_solutions[i]['output'])\n                predicted_grid = interpreter.run(program_ast, inp_grid)\n                \n                if not np.array_equal(predicted_grid, expected_output_grid):\n                    return False\n            return True\n        except Exception as e:\n            return False\n\n    def query_ltm_cache(self, novel_fingerprint: np.ndarray) -> List[Tuple[List[Dict], str, float]]:\n        \"\"\" Performs a k-Nearest Neighbor (k-NN) search \"\"\"\n        profiler.start(\"query_ltm_cache\")\n        \n        results = []\n        if self.ltm_vectors is None or novel_fingerprint is None or len(self.ltm_programs) == 0:\n            profiler.end(\"query_ltm_cache\")\n            return results\n            \n        try:\n            distances = np.linalg.norm(self.ltm_vectors - novel_fingerprint, axis=1)\n            k = self.config.LTM_CACHE_K\n            nearest_indices = np.argsort(distances)[:k]\n            \n            for i in nearest_indices:\n                program_ast = self.ltm_programs[i]\n                dist = distances[i]\n                name = f\"LTM_Abduction_k{i}_dist{dist:.2f}\"\n                results.append( (program_ast, name, dist) )\n                    \n        except Exception as e:\n            pass\n            \n        profiler.end(\"query_ltm_cache\")\n        return results\n\n    def update_ltm_cache(self, fingerprint: np.ndarray, program_ast: List[Dict]):\n        \"\"\" Performs \"Online Learning\". \"\"\"\n        profiler.start(\"update_ltm_cache\")\n        try:\n            self._ltm_vectors_list.append(fingerprint)\n            self.ltm_vectors = np.stack(self.ltm_vectors_list)\n            self.ltm_programs.append(program_ast)\n        except Exception as e:\n            pass\n        profiler.end(\"update_ltm_cache\")\n\n    def run_triage_phase(self, tasks_to_analyze: Dict[str, Dict]) -> Counter:\n        \"\"\"\n        Executes \"Phase 1 Triage\" on the (unknown) TEST SET.\n        (Unchanged from HOTFIX 11)\n        \"\"\"\n        print(\"\\n--- ðŸ§  EXECUTING PHASE 1: HEURISTIC TRIAGE (on test set) ---\")\n        if not tasks_to_analyze:\n            print(\"  âŒ No tasks to analyze. Triage skipped.\")\n            return Counter()\n            \n        profiler.start(\"Phase1_Triage_Full\")\n        \n        tasks_to_actually_triage = tasks_to_analyze\n        if self.config.DIAGNOSTIC_RUN:\n            print(f\"  *** âš ï¸  DIAGNOSTIC MODE: Triaging {self.config.DIAGNOSTIC_SAMPLE_SIZE} tasks... ***\")\n            task_keys_to_triage = list(tasks_to_analyze.keys())[:self.config.DIAGNOSTIC_SAMPLE_SIZE]\n            tasks_to_actually_triage = {k: tasks_to_analyze[k] for k in task_keys_to_triage}\n        \n        for task_id, task_data in tasks_to_actually_triage.items():\n            profile = self.solver_toolbox.analyzer.analyze(task_data, task_id)\n            self.task_profiles[task_id] = profile\n            \n        profiler.end(\"Phase1_Triage_Full\")\n        \n        stats = Counter([p.difficulty_tier for p in self.task_profiles.values()])\n        print(f\"  âœ… Triage complete. Task profile generated for all {len(self.task_profiles)} tasks.\")\n        print(f\"  Triage Stats: {stats['easy']} Easy | {stats['medium']} Medium | {stats['hard']} Hard\")\n        return stats\n\n    def allocate_time_budgets(self, total_solve_budget: float, tier_counts: Counter):\n        \"\"\"\n        *** HOTFIX 16: Implements \"60-second punt\" for INFERENCE. ***\n        \"\"\"\n        print(\"\\n--- â³ Allocating time budgets (Î”H Modulation) ---\")\n        \n        total_abstraction_budget = total_solve_budget * self.config.abstraction_pass_time_ratio\n        total_reasoning_budget = total_solve_budget * self.config.reasoning_pass_time_ratio\n        \n        print(f\"  Abstraction Pass (Î”H-) Total Budget: {total_abstraction_budget/3600:.2f} hours\")\n        print(f\"  Reasoning Pass (Î”H+) Total Budget: {total_reasoning_budget/3600:.2f} hours\")\n\n        total_tasks = len(self.task_profiles)\n        if total_tasks == 0:\n            print(\"  âŒ No tasks found. Cannot allocate time.\")\n            return\n\n        # --- 1. Identify the 9 \"Punt\" Tasks (3 easy, 3 med, 3 hard) ---\n        sorted_profiles = sorted(self.task_profiles.values(), key=lambda p: p.difficulty_score)\n        \n        punt_tasks_easy = [p.task_id for p in sorted_profiles if p.difficulty_tier == 'easy'][:3]\n        punt_tasks_medium = [p.task_id for p in sorted_profiles if p.difficulty_tier == 'medium'][:3]\n        punt_tasks_hard = [p.task_id for p in sorted_profiles if p.difficulty_tier == 'hard'][:3]\n        \n        punt_task_set = set(punt_tasks_easy + punt_tasks_medium + punt_tasks_hard)\n        print(f\"  Identified {len(punt_task_set)} tasks for 2x budget 'punt' (60s).\")\n\n        # --- 2. Calculate \"Shaved\" Budget (MDMP) ---\n        weights = {'easy': 1, 'medium': 3, 'hard': 6}\n        punt_timeout = self.config.PUNT_TASK_BUDGET_SECONDS # 60.0s\n        \n        punt_cost_reasoning = len(punt_task_set) * punt_timeout\n        punt_cost_abstraction = len(punt_task_set) * 10.0 # 10s for LTM/HPN check\n        \n        remaining_budget_reasoning = total_reasoning_budget - punt_cost_reasoning\n        remaining_budget_abstraction = total_abstraction_budget - punt_cost_abstraction\n        \n        remaining_weighted_units = 0\n        for task_id, profile in self.task_profiles.items():\n            if task_id not in punt_task_set:\n                remaining_weighted_units += weights.get(profile.difficulty_tier, 1)\n\n        if remaining_weighted_units <= 0: remaining_weighted_units = 1\n        \n        unit_time_abstraction = remaining_budget_abstraction / remaining_weighted_units\n        unit_time_reasoning = remaining_budget_reasoning / remaining_weighted_units\n        \n        if unit_time_reasoning < 1.0 or unit_time_abstraction < 1.0:\n            print(f\"  âš ï¸  WARNING: 60s punt tasks consumed entire budget (Remaining: {remaining_budget_reasoning:.0f}s). \"\n                  f\"All other tasks will get 1s.\")\n            unit_time_abstraction = max(1.0, unit_time_abstraction)\n            unit_time_reasoning = max(1.0, unit_time_reasoning)\n\n        # --- 4. Assign Budgets (Punt or Shaved) ---\n        for task_id, profile in self.task_profiles.items():\n            if task_id in punt_task_set:\n                self.time_allocations['abstraction_per_task'][task_id] = 10.0\n                self.time_allocations['reasoning_per_task'][task_id] = punt_timeout\n            else:\n                base_weight = weights.get(profile.difficulty_tier, 1)\n                self.time_allocations['abstraction_per_task'][task_id] = unit_time_abstraction * base_weight\n                self.time_allocations['reasoning_per_task'][task_id] = unit_time_reasoning * base_weight\n        \n        print(\"  âœ… Time budgets allocated per task (60s Punt Strategy).\")\n        std_easy_task_id = next((p.task_id for p in sorted_profiles if p.difficulty_tier == 'easy' and p.task_id not in punt_task_set), None)\n        if std_easy_task_id:\n             print(f\"  Time (Std. Easy, Rea): {self.time_allocations['reasoning_per_task'].get(std_easy_task_id, 0.0):.1f}s\")\n        if punt_tasks_easy:\n            print(f\"  Time (Punt Easy, Rea): {self.time_allocations['reasoning_per_task'].get(punt_tasks_easy[0], 0.0):.1f}s (PUNT)\")\n        \n        std_hard_task_id = next((p.task_id for p in reversed(sorted_profiles) if p.difficulty_tier == 'hard' and p.task_id not in punt_task_set), None)\n        if std_hard_task_id:\n             print(f\"  Time (Std. Hard, Rea): {self.time_allocations['reasoning_per_task'].get(std_hard_task_id, 0.0):.1f}s\")\n        if punt_tasks_hard:\n             print(f\"  Time (Punt Hard, Rea): {self.time_allocations['reasoning_per_task'].get(punt_tasks_hard[0], 0.0):.1f}s (PUNT)\")\n\n\n# --- 2. Main Initialization & \"Game Genie\" Execution ---\n\nnotebook_start_time = time.time()\nsetup_memory_limits() # From Cell 2\n\n# Instantiate the Master Controller, which builds the whole AGI (Cell 7)\nprofiler.start(\"Total_Initialization\")\ncontroller = RSC_Controller(CONFIG)\nprofiler.end(\"Total_Initialization\")\n\n\n# --- THIS IS THE LTM-v4 \"TRAINING\" STEP ---\nif ('training_tasks' in locals() and 'evaluation_tasks' in locals() and\n    'training_solutions' in locals() and 'evaluation_solutions' in locals()):\n    \n    all_known_tasks = {**training_tasks, **evaluation_tasks}\n    all_known_solutions = {**training_solutions, **evaluation_solutions}\n    \n    tasks_to_train_on = all_known_tasks\n    solutions_to_train_on = all_known_solutions\n    \n    if CONFIG.DIAGNOSTIC_RUN:\n        print(\"--- âš ï¸  DIAGNOSTIC MODE: Sampling training data... ---\")\n        sample_size = CONFIG.DIAGNOSTIC_SAMPLE_SIZE\n        sample_keys = list(all_known_tasks.keys())[:sample_size]\n        \n        tasks_to_train_on = {k: all_known_tasks[k] for k in sample_keys}\n        solutions_to_train_on = {k: all_known_solutions[k] for k in sample_keys if k in all_known_solutions}\n        print(f\"--- âš ï¸  Running 'Game Genie' on {len(tasks_to_train_on)} tasks. ---\")\n\n    controller.run_pre_computation(\n        tasks_to_train_on,\n        solutions_to_train_on\n    )\nelse:\n    print(\"\\nâš ï¸  Missing training/evaluation data. Skipping 'Game Genie' LTM-v4 training.\")\n\n\n# --- Run Phase 1 Triage on the TEST SET ---\nif test_tasks:\n    tier_stats = controller.run_triage_phase(test_tasks)\n    \n    init_time_taken = time.time() - notebook_start_time\n    total_budget = CONFIG.total_time_budget\n    submission_buffer = CONFIG.submission_buffer\n    \n    solve_budget = total_budget - init_time_taken - submission_buffer\n    \n    if CONFIG.DIAGNOSTIC_RUN:\n        min_runtime_seconds = CONFIG.DIAGNOSTIC_MIN_RUNTIME_MINUTES * 60\n        # Enforce 30-min floor\n        if (init_time_taken + solve_budget + submission_buffer) < min_runtime_seconds:\n            solve_budget = max(0.0, min_runtime_seconds - init_time_taken - submission_buffer)\n            print(f\"  DIAGNOSTIC 30-min floor: New Solve Budget is {solve_budget/60:.2f} minutes.\")\n    \n    print(f\"\\nInitialization & LTM-v4 Training took {init_time_taken / 60:.2f} minutes.\")\n    print(f\"Total Solve Budget Remaining: {solve_budget/3600:.2f} hours.\")\n\n    controller.allocate_time_budgets(solve_budget, tier_stats)\nelse:\n    print(\"\\nâŒ No test tasks loaded. Skipping Triage and Time Allocation.\")\n    solve_budget = 0 \n\nprint(\"=\"*70)\n#Cell 9\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:41:35.185327Z","iopub.execute_input":"2025-11-08T20:41:35.185706Z","iopub.status.idle":"2025-11-08T20:43:54.989169Z","shell.execute_reply.started":"2025-11-08T20:41:35.185678Z","shell.execute_reply":"2025-11-08T20:43:54.988295Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nðŸŒŠâš›ï¸ LucidOrca Solver: Cell 9 (LTM-v4) RSC Controller & LTM Trainer [HOTFIX 19]\nðŸ§  Memory limit set: 12.00 GB\nâ™»ï¸  Garbage collection: ENABLED (aggressive mode)\n\nInitializing RSC_Controller (LTM-v4)...\n  âœ… MetricLogger initialized. Writing to /kaggle/working/lucid_metrics.csv\n  Bootstrapping Cognitive Architecture (Phase 1)...\n\n======================================================================\nðŸŒŠâš›ï¸ Initializing LucidOrcaUltimateSolver (LTM-v4 Master Toolbox)...\n  PerceptionEngine (LTM-v4 'Vision') initialized.\n  âœ… 1. PerceptionEngine (Vision)... LOADED\n  VisionModelEncoder (LTM-v4) initialized. Fingerprint dim: 28\n  TaskFingerprinter (Task-to-Delta-Vector Engine) initialized.\n  TaskAnalyzer (Phase 1 Triage Engine) initialized.\n  âœ… 2. TaskAnalyzer (Triage & Fingerprinting)... LOADED\n  MetaPrimitives (LTM-v4 'ALU') initialized. [HOTFIX 15]\n  SymbolicProgramInterpreter (LTM-v4 'CPU') initialized. [HOTFIX 19]\n  âœ… 3. SymbolicProgramInterpreter ('CPU')... LOADED\n  HeuristicPlaybookSolver (LTM-v4 'Fast Brain') initialized. [HPN *NOT* LOADED]\n  âœ… 4. HeuristicPlaybookSolver ('Fast Brain')... LOADED\n  SymbolicProgramSynthesizer (LTM-v4 'Slow Brain') initialized. [HOTFIX 19 - HPN *NOT* LOADED]\n  âœ… 5. SymbolicProgramSynthesizer ('Schooled Slow Brain')... LOADED\n  SolverAgreementEnsemble (Consensus Utility) initialized.\n  âœ… 6. Consensus & Utility Modules... LOADED\n\nâœ… LucidOrca LTM-v4 Solver (Toolbox) is ready!\n======================================================================\n  Bootstrapping Cognitive Architecture (Phase 2)...\n  âœ… Cognitive Bootcamp complete. Built HPN Grammar (8 rules) and HPN Playbook (8 seeds).\n\n======================================================================\nðŸŒŠâš›ï¸ Initializing LucidOrcaUltimateSolver (LTM-v4 Master Toolbox)...\n  PerceptionEngine (LTM-v4 'Vision') initialized.\n  âœ… 1. PerceptionEngine (Vision)... LOADED\n  VisionModelEncoder (LTM-v4) initialized. Fingerprint dim: 28\n  TaskFingerprinter (Task-to-Delta-Vector Engine) initialized.\n  TaskAnalyzer (Phase 1 Triage Engine) initialized.\n  âœ… 2. TaskAnalyzer (Triage & Fingerprinting)... LOADED\n  MetaPrimitives (LTM-v4 'ALU') initialized. [HOTFIX 15]\n  SymbolicProgramInterpreter (LTM-v4 'CPU') initialized. [HOTFIX 19]\n  âœ… 3. SymbolicProgramInterpreter ('CPU')... LOADED\n  HeuristicPlaybookSolver (LTM-v4 'Fast Brain') initialized. [HPN LOADED]\n  âœ… 4. HeuristicPlaybookSolver ('Fast Brain')... LOADED\n  SymbolicProgramSynthesizer (LTM-v4 'Slow Brain') initialized. [HOTFIX 19 - HPN LOADED]\n  âœ… 5. SymbolicProgramSynthesizer ('Schooled Slow Brain')... LOADED\n  SolverAgreementEnsemble (Consensus Utility) initialized.\n  âœ… 6. Consensus & Utility Modules... LOADED\n\nâœ… LucidOrca LTM-v4 Solver (Toolbox) is ready!\n======================================================================\n  âœ… RSC_Controller is online (LTM-v4 Architecture).\n--- âš ï¸  DIAGNOSTIC MODE: Sampling training data... ---\n--- âš ï¸  Running 'Game Genie' on 100 tasks. ---\n\n--- ðŸ§  EXECUTING PRE-COMPUTATION: 'GAME GENIE' LTM-v4 TRAINING ---\n  *** âš ï¸  DIAGNOSTIC MODE: Training on 100 tasks... ***\n  Scaled LTM Budget: 30.00 minutes (30-min floor enforced)\n  Phase 0: Triaging all tasks for curriculum...\n  Budgeting: Standard Timeout = 13.33s | Punt Timeout = 60.00s\n\n  --- Phase A: Solving 10 Easiest Tasks (60s Budget) ---\n  Punt Task 1/10: 1478ab18 (easy) -> Synthesizer.Fail.MaxDepth. (Cached: False)\n  Punt Task 2/10: 1c0d0a4b (easy) -> Synthesizer.Fail.MaxDepth. (Cached: False)\n  Punt Task 3/10: 1caeab9d (easy) -> Synthesizer.Fail.MaxDepth. (Cached: False)\n  Punt Task 4/10: 12eac192 (easy) -> Synthesizer.Fail.MaxDepth. (Cached: False)\n  Punt Task 5/10: 1b60fb0c (easy) -> Synthesizer.Fail.MaxDepth. (Cached: False)\n  Punt Task 6/10: 17cae0c1 (easy) -> Synthesizer.Fail.MaxDepth. (Cached: False)\n  Punt Task 7/10: 00d62c1b (easy) -> Synthesizer.Fail.MaxDepth. (Cached: False)\n  Punt Task 8/10: 05f2a901 (easy) -> Synthesizer.Fail.MaxDepth. (Cached: False)\n  Punt Task 9/10: 150deff5 (easy) -> Synthesizer.Fail.MaxDepth. (Cached: False)\n  Punt Task 10/10: 0d3d703e (easy) -> Synthesizer.Fail.MaxDepth. (Cached: False)\n\n  --- Phase B: Solving 90 Remaining Tasks (13.33s Budget) ---\n  LTM Training Progress... Task 30/100 processed. (0 programs cached)\n  LTM Training Progress... Task 40/100 processed. (0 programs cached)\n  LTM Training Progress... Task 50/100 processed. (0 programs cached)\n  LTM Training Progress... Task 60/100 processed. (0 programs cached)\n  LTM Training Progress... Task 70/100 processed. (0 programs cached)\n  LTM Training Progress... Task 80/100 processed. (0 programs cached)\n  LTM Training Progress... Task 90/100 processed. (0 programs cached)\n  LTM Training Progress... Task 100/100 processed. (0 programs cached)\n  LTM Training Progress... Task 110/100 processed. (0 programs cached)\n\n  âš ï¸  'Game Genie' LTM-v4 Training FAILED. No programs were cached.\n\n--- ðŸ§  EXECUTING PHASE 1: HEURISTIC TRIAGE (on test set) ---\n  *** âš ï¸  DIAGNOSTIC MODE: Triaging 100 tasks... ***\n  âœ… Triage complete. Task profile generated for all 100 tasks.\n  Triage Stats: 16 Easy | 70 Medium | 14 Hard\n\nInitialization & LTM-v4 Training took 2.33 minutes.\nTotal Solve Budget Remaining: 7.71 hours.\n\n--- â³ Allocating time budgets (Î”H Modulation) ---\n  Abstraction Pass (Î”H-) Total Budget: 2.31 hours\n  Reasoning Pass (Î”H+) Total Budget: 5.40 hours\n  Identified 9 tasks for 2x budget 'punt' (60s).\n  âœ… Time budgets allocated per task (60s Punt Strategy).\n  Time (Std. Easy, Rea): 67.5s\n  Time (Punt Easy, Rea): 60.0s (PUNT)\n  Time (Std. Hard, Rea): 404.8s\n  Time (Punt Hard, Rea): 60.0s (PUNT)\n======================================================================\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#Cell 10\n################################################################################\n#\n# ðŸŒŠâš›ï¸ LUCIDORCA ULTIMATE SOLVER - (LTM-v4 REBUILD)\n#\n# Cell 10: Main Inference, Validation, and Save\n#\n# *** LTM-v4 HOTFIX 16 (The \"Two-Brain\" Solution): ***\n#\n# 1. The main inference loop is re-architected to use our \"Two-Brain\"\n#    model, separating \"Abstraction\" from \"Reasoning\".\n# 2. Phase 2 (Abstraction): We now call the new \"Fast Brain\"\n#    (`heuristic_solver`). This runs the HPN Playbook k-NN search.\n# 3. Phase 3 (Reasoning): *Only if Phase 2 fails*, we call the\n#    \"Slow Brain\" (`synthesizer`) to run its brute-force search.\n# 4. This is a true \"Heuristics-first\" cognitive architecture.\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(\"ðŸŒŠâš›ï¸ LucidOrca Solver: Cell 10 (LTM-v4) - Main Inference Loop [HOTFIX 16]\")\n\n# --- 1. Setup Execution ---\n\nsubmission = {}\nDEFAULT_PROGRAM_AST = []\n\ndef _generate_variation_grid(grid: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Generates a *different* grid to be used for `attempt_2`\n    if both solver passes return the *exact same* grid.\n    \"\"\"\n    if grid is None or grid.size == 0:\n        return np.array([[0]])\n\n    try:\n        variation_1 = np.rot90(grid, 1)\n        if not np.array_equal(variation_1, grid):\n            return variation_1\n        \n        variation_2 = np.fliplr(grid)\n        if not np.array_equal(variation_2, grid):\n            return variation_2\n            \n        return np.rot90(grid, 2)\n        \n    except Exception:\n        return np.array([[0]])\n\n\ndef _validate_program_on_train(program_ast: List[Dict], \n                             task_data: Dict, \n                             interpreter: SymbolicProgramInterpreter) -> bool:\n    \"\"\"\n    Validates a \"best guess\" program (from LTM) against the\n    novel task's *own* training examples.\n    \"\"\"\n    if program_ast is None:\n        return False\n        \n    examples = task_data.get('train', [])\n    if not examples:\n        return False\n\n    try:\n        for ex in examples:\n            inp_grid = np.array(ex['input'])\n            expected_output_grid = np.array(ex['output'])\n            predicted_grid = interpreter.run(program_ast, inp_grid)\n            \n            if not np.array_equal(predicted_grid, expected_output_grid):\n                return False\n        return True\n    except Exception:\n        return False\n\n\n# Check that the controller and tasks from previous cells are loaded\nif 'controller' not in locals() or 'test_tasks' not in locals() or not test_tasks:\n    print(\"âŒ CRITICAL ERROR: `controller` or `test_tasks` not found.\")\n    print(\"   Please re-run Cell 8 and Cell 9 before this one.\")\n    \nelse:\n    # --- 2. Get All AGI Components from Controller ---\n    \n    solver_toolbox = controller.solver_toolbox\n    interpreter = solver_toolbox.interpreter\n    \n    # --- *** NEW (HOTFIX 16): Get *both* brains *** ---\n    fast_brain_solver = solver_toolbox.heuristic_solver # \"The Architect\"\n    slow_brain_solver = solver_toolbox.synthesizer      # \"The Searcher\"\n    \n    task_profiles = controller.task_profiles\n    time_allocations = controller.time_allocations\n    \n    total_solve_budget = solve_budget \n    main_loop_start_time = time.time()\n\n    sorted_task_ids = list(task_profiles.keys())\n    total_tasks_count = len(sorted_task_ids)\n    \n    print(f\"  Starting main LTM-v4 inference loop for {total_tasks_count} tasks...\")\n    if CONFIG.DIAGNOSTIC_RUN:\n        print(f\"  *** âš ï¸  DIAGNOSTIC MODE: Inference run is limited to {total_tasks_count} tasks. ***\")\n    print(f\"  Total Solve Budget: {total_solve_budget/3600:.2f} hours\")\n    print(\"=\"*70)\n\n    # --- HOTFIX 9: Write new header for Inference phase ---\n    controller.metric_logger.write_header(controller.INFERENCE_LOG_COLUMNS)\n\n    # --- 3. Main Execution Loop (LTM-v4 Inference) ---\n    \n    for i, task_id in enumerate(sorted_task_ids):\n        task_start_time = time.time()\n        \n        task_data = test_tasks[task_id]\n        profile = task_profiles[task_id]\n        \n        elapsed_total = time.time() - main_loop_start_time\n        if elapsed_total > total_solve_budget:\n            print(f\"\\nâ±ï¸  MASTER TIME BUDGET EXCEEDED. Stopping solve loop.\")\n            print(f\"  Completed {i}/{total_tasks_count} tasks.\")\n            break\n            \n        print(f\"\\n--- [ {i+1}/{total_tasks_count} ] Solving Task: {task_id} (Tier: {profile.difficulty_tier}, Basin: {profile.basin}) ---\")\n        \n        program_LTM = None # \"Abstraction\" (LTM k-NN)\n        program_Heuristic = None # \"Architect\" (HPN Playbook)\n        program_DeepSearch = None # \"Searcher\" (Brute-Force)\n\n        ltm_status = \"Miss\"\n        heuristic_status = \"Fail.NotRun\"\n        reasoning_status = \"Fail.NotRun\"\n        \n        # --- C. Execute Phase 2: Abstraction (LTM k-NN) ---\n        # This is our *real* LTM, which we are *finally* populating.\n        profiler.start(f\"Phase2_LTM_Query.{task_id}\")\n        \n        novel_fingerprint = profile.delta_fingerprint\n        \n        if novel_fingerprint is None:\n            print(\"  âš ï¸  Task fingerprinting failed. Skipping LTM query.\")\n            ltm_status = \"Fail.NoFingerprint\"\n        else:\n            print(\"  Querying LTM-v4 (k-NN)...\\n  (This is the *real* LTM, not the HPN)\")\n            candidates = controller.query_ltm_cache(novel_fingerprint)\n            \n            if candidates:\n                print(f\"  LTM-v4 returned {len(candidates)} candidates. Starting Sanity Check...\")\n                for (program_ast, rule_name, dist) in candidates:\n                    if _validate_program_on_train(program_ast, task_data, interpreter):\n                        print(f\"  âœ… Abduction PASSED Sanity Check! (Found {rule_name})\")\n                        program_LTM = program_ast\n                        ltm_status = \"Hit\"\n                        controller.update_ltm_cache(novel_fingerprint, program_LTM)\n                        break \n                if program_LTM is None:\n                    print(\"  âš ï¸  LTM Abduction FAILED Sanity Check on all candidates.\")\n                    ltm_status = \"Fail.SanityCheck\"\n            else:\n                print(\"  LTM-v4 Cache Miss. No similar task found.\")\n                ltm_status = \"Miss\"\n        profiler.end(f\"Phase2_LTM_Query.{task_id}\")\n\n        # --- D. Execute Phase 3a: Heuristics (\"Fast Brain\") ---\n        # If the *real* LTM missed, we try our \"Fast Brain\" architect.\n        if program_LTM is None:\n            profiler.start(f\"Phase3a_HeuristicSolver.{task_id}\")\n            print(\"  Executing Phase 3a (Î”H-) Fast-Brain Architect...\")\n            \n            abstraction_budget = time_allocations['abstraction_per_task'][task_id]\n            (program_ast, rule_name) = fast_brain_solver.solve(task_data, timeout=abstraction_budget)\n            \n            if program_ast is not None:\n                program_Heuristic = program_ast\n                heuristic_status = rule_name # e.g., \"Playbook.Success\"\n                print(f\"  âœ… Phase 3a (Î”H-) Fast-Brain SUCCESS: [{heuristic_status}]\")\n            else:\n                heuristic_status = rule_name # e.g., \"Playbook.Fail.NoMatch\"\n                print(f\"  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [{heuristic_status}]\")\n            profiler.end(f\"Phase3a_HeuristicSolver.{task_id}\")\n\n        # --- E. Execute Phase 3b: Reasoning (\"Slow Brain\") ---\n        # If *both* LTM and Fast Brain failed, we fall back to brute-force.\n        if program_LTM is None and program_Heuristic is None:\n            profiler.start(f\"Phase3b_DeepSearch.{task_id}\")\n            print(\"  Executing Phase 3b (Î”H+) Slow-Brain Search...\")\n            \n            reasoning_budget = time_allocations['reasoning_per_task'][task_id]\n            (program_ast, rule_name) = slow_brain_solver.solve(task_data, timeout=reasoning_budget)\n            \n            if program_ast is not None:\n                program_DeepSearch = program_ast\n                reasoning_status = rule_name # e.g., \"Synthesizer.Success.d3\"\n                print(f\"  âœ… Phase 3b (Î”H+) Slow-Brain SUCCESS: [{reasoning_status}]\")\n            else:\n                reasoning_status = rule_name # e.g., \"Synthesizer.Fail.MaxDepth\"\n                print(f\"  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [{reasoning_status}]\")\n            profiler.end(f\"Phase3b_DeepSearch.{task_id}\")\n\n\n        # --- F. RSC Arbiter: Select Final Programs ---\n        final_program_1 = program_LTM or program_Heuristic or program_DeepSearch or DEFAULT_PROGRAM_AST\n        final_program_2 = program_DeepSearch or program_Heuristic or program_LTM or DEFAULT_PROGRAM_AST\n        \n        final_program_source = \"Fallback\"\n        if program_LTM: final_program_source = \"LTM\"\n        elif program_Heuristic: final_program_source = \"Heuristic\"\n        elif program_DeepSearch: final_program_source = \"DeepSearch\"\n        \n        \n        # --- G. CRITICAL: Apply Programs to ALL Test Cases ---\n        task_solutions = [] \n        num_expected_outputs = len(task_data.get('test', []))\n        \n        if num_expected_outputs == 0:\n            print(f\"  âŒ Task {task_id} has no test cases. Skipping.\")\n            continue\n            \n        print(f\"  Applying programs to {num_expected_outputs} test case(s)...\")\n        \n        for test_case_index in range(num_expected_outputs):\n            try:\n                test_input_grid = np.array(task_data['test'][test_case_index]['input'])\n            except Exception:\n                test_input_grid = np.array([[0]]) \n            \n            try:\n                grid_1 = interpreter.run(final_program_1, test_input_grid)\n            except Exception:\n                grid_1 = test_input_grid\n                \n            try:\n                grid_2 = interpreter.run(final_program_2, test_input_grid)\n            except Exception:\n                grid_2 = test_input_grid\n                \n            if np.array_equal(grid_1, grid_2):\n                grid_2 = _generate_variation_grid(grid_1)\n                \n            task_solutions.append({\n                \"attempt_1\": grid_1.tolist(),\n                \"attempt_2\": grid_2.tolist()\n            })\n\n        # --- H. Store Final Solutions for this Task ---\n        submission[task_id] = task_solutions\n        task_time = time.time() - task_start_time\n        print(f\"  âž¡ï¸  Task {task_id} finished in {task_time:.2f}s. Stored {len(task_solutions)} solutions.\")\n\n        # --- HOTFIX 9: Log Inference Metrics ---\n        log_data = {\n            'columns_order': controller.INFERENCE_LOG_COLUMNS,\n            'phase': \"Inference\",\n            'task_id': task_id,\n            'task_tier': profile.difficulty_tier,\n            'basin': profile.basin,\n            'ltm_status': ltm_status,\n            'reasoning_status': f\"{heuristic_status} | {reasoning_status}\",\n            'final_program': final_program_source,\n            'time_taken_s': f\"{task_time:.3f}\"\n        }\n        controller.metric_logger.log(log_data)\n\n\n    # --- 4. Final Summary of Loop ---\n    \n    total_solve_time = time.time() - main_loop_start_time\n    print(\"\\n\" + \"=\"*70)\n    print(\"âœ… MAIN INFERENCE LOOP COMPLETE\")\n    print(f\"  Total tasks processed: {len(submission)} / {total_tasks_count}\")\n    print(f\"  Total Solve Time: {total_solve_time / 60:.2f} minutes\")\n    profiler.print_summary()\n\n\n# --- 5. Final Validation & Sanitization ---\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ðŸŒŠâš›ï¸ Final Validation & Sanitization Pass\")\nprint(\"=\"*70)\n\ndef generate_compliant_fallback(task_id: str, task_data: Dict) -> List[Dict]:\n    \"\"\"\n    Generates a compliant fallback solution (copies input)\n    for a given task_id.\n    \"\"\"\n    try:\n        fallback_grid = np.array(task_data['test'][0]['input'])\n    except Exception:\n        fallback_grid = np.array([[0]]) \n\n    num_expected_outputs = len(task_data.get('test', []))\n    if num_expected_outputs == 0:\n        return [] \n\n    all_test_solutions = []\n    for test_case_index in range(num_expected_outputs):\n        try:\n            input_grid = np.array(task_data['test'][test_case_index]['input'])\n        except Exception:\n            input_grid = fallback_grid \n            \n        attempt_1_grid = input_grid\n        attempt_2_grid = _generate_variation_grid(attempt_1_grid)\n        \n        all_test_solutions.append({\n            \"attempt_1\": attempt_1_grid.tolist(),\n            \"attempt_2\": attempt_2_grid.tolist()\n        })\n    \n    return all_test_solutions\n\n\nvalidation_passed = True\ntasks_overwritten = 0\ntasks_missing = 0\n\nif not test_tasks:\n    print(\"âŒ No test tasks loaded. Cannot validate or save.\")\n    validation_passed = False\nelse:\n    print(f\"  Validating submission against all {len(test_tasks)} required tasks...\")\n    \n    for task_id, task_data in test_tasks.items():\n        \n        num_expected_outputs = len(task_data.get('test', []))\n        if num_expected_outputs == 0:\n            continue \n\n        is_valid = True \n\n        if task_id not in submission:\n            tasks_missing += 1\n            is_valid = False\n            \n        else:\n            task_outputs_list = submission[task_id]\n            \n            if not isinstance(task_outputs_list, list): is_valid = False\n            elif len(task_outputs_list) != num_expected_outputs: is_valid = False\n            else:\n                for item in task_outputs_list:\n                    if not isinstance(item, dict): is_valid = False; break\n                    if \"attempt_1\" not in item or \"attempt_2\" not in item: is_valid = False; break\n                    if not isinstance(item['attempt_1'], list) or not isinstance(item['attempt_2'], list): is_valid = False; break\n\n        if not is_valid:\n            if task_id in submission:\n                print(f\"  âŒ Task {task_id} has FORMAT ERROR. Overwriting with compliant fallback.\")\n                tasks_overwritten += 1\n            \n            submission[task_id] = generate_compliant_fallback(task_id, task_data)\n            validation_passed = False\n\nif tasks_missing > 0:\n    print(f\"\\n! VALIDATION WARNING: {tasks_missing} tasks were missing and replaced with fallbacks.\")\nif tasks_overwritten > 0:\n    print(f\"\\n! VALIDATION ERROR: {tasks_overwritten} tasks had format errors and were overwritten.\")\nif validation_passed and tasks_missing == 0 and tasks_overwritten == 0:\n    print(\"\\nâœ…âœ…âœ… SUBMISSION FORMAT IS 100% VALID! âœ…âœ…âœ…\")\nelse:\n    print(\"\\nâš ï¸  Submission contains fallbacks or errors, but is now 100% format-compliant.\")\n\n\n# --- 6. Save the Sanitized Submission ---\n\nOUTPUT_PATH = Path(\"/kaggle/working/submission.json\")\nprint(f\"\\nðŸ’¾ Saving sanitized submission to: {OUTPUT_PATH}\")\n\ntry:\n    with open(OUTPUT_PATH, 'w') as f:\n        json.dump(submission, f, separators=(',', ':'))\n\n    file_size = OUTPUT_PATH.stat().st_size\n    print(f\"   âœ… Saved {len(submission)} tasks ({file_size/1024:.1f} KB)\")\n\nexcept Exception as e:\n    print(f\"âŒ CRITICAL ERROR: FAILED TO SAVE SUBMISSION: {e}\")\n\n# --- 7. Final Notebook Summary ---\n\ntotal_notebook_time = time.time() - notebook_start_time\nprint(\"\\n\" + \"=\"*70)\nprint(\"ðŸ†ðŸ NOTEBOOK EXECUTION COMPLETE ðŸðŸ†\")\nprint(\"=\"*70)\nprint(f\"  Total Tasks in Submission: {len(submission)} / {len(test_tasks)}\")\nprint(f\"  Total Notebook Runtime: {total_notebook_time / 60:.2f} minutes\")\nprint(f\"  (Total Budget: {CONFIG.total_time_budget / 60:.2f} minutes)\")\nprint(f\"\\n  Final File: {OUTPUT_PATH}\")\nif 'file_size' in locals():\n    print(f\"  Final Size: {file_size/1024:.1f} KB\")\n\n# --- HOTFIX 9: Close the logger ---\nif 'controller' in locals() and hasattr(controller, 'metric_logger'):\n    controller.metric_logger.close()\n\nprint(\"\\n  Ready for Kaggle submission.\")\nprint(\"=\"*70)\n#Cell 10\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:43:54.991630Z","iopub.execute_input":"2025-11-08T20:43:54.991904Z","iopub.status.idle":"2025-11-08T20:46:20.223808Z","shell.execute_reply.started":"2025-11-08T20:43:54.991885Z","shell.execute_reply":"2025-11-08T20:46:20.222746Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nðŸŒŠâš›ï¸ LucidOrca Solver: Cell 10 (LTM-v4) - Main Inference Loop [HOTFIX 16]\n  Starting main LTM-v4 inference loop for 100 tasks...\n  *** âš ï¸  DIAGNOSTIC MODE: Inference run is limited to 100 tasks. ***\n  Total Solve Budget: 7.71 hours\n======================================================================\n\n--- [ 1/100 ] Solving Task: 00576224 (Tier: hard, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 00576224 finished in 0.40s. Stored 1 solutions.\n\n--- [ 2/100 ] Solving Task: 007bbfb7 (Tier: hard, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 007bbfb7 finished in 0.82s. Stored 1 solutions.\n\n--- [ 3/100 ] Solving Task: 009d5c81 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 009d5c81 finished in 0.52s. Stored 1 solutions.\n\n--- [ 4/100 ] Solving Task: 00d62c1b (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 00d62c1b finished in 0.32s. Stored 1 solutions.\n\n--- [ 5/100 ] Solving Task: 00dbd492 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 00dbd492 finished in 0.40s. Stored 1 solutions.\n\n--- [ 6/100 ] Solving Task: 017c7c7b (Tier: medium, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 017c7c7b finished in 1.69s. Stored 1 solutions.\n\n--- [ 7/100 ] Solving Task: 025d127b (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 025d127b finished in 0.39s. Stored 1 solutions.\n\n--- [ 8/100 ] Solving Task: 03560426 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 03560426 finished in 0.85s. Stored 1 solutions.\n\n--- [ 9/100 ] Solving Task: 045e512c (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 045e512c finished in 0.40s. Stored 1 solutions.\n\n--- [ 10/100 ] Solving Task: 0520fde7 (Tier: medium, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0520fde7 finished in 1.38s. Stored 1 solutions.\n\n--- [ 11/100 ] Solving Task: 05269061 (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 05269061 finished in 0.91s. Stored 1 solutions.\n\n--- [ 12/100 ] Solving Task: 05a7bcf2 (Tier: hard, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 05a7bcf2 finished in 0.70s. Stored 1 solutions.\n\n--- [ 13/100 ] Solving Task: 05f2a901 (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 05f2a901 finished in 0.46s. Stored 1 solutions.\n\n--- [ 14/100 ] Solving Task: 0607ce86 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0607ce86 finished in 3.77s. Stored 1 solutions.\n\n--- [ 15/100 ] Solving Task: 0692e18c (Tier: hard, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0692e18c finished in 0.41s. Stored 1 solutions.\n\n--- [ 16/100 ] Solving Task: 06df4c85 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 06df4c85 finished in 0.79s. Stored 1 solutions.\n\n--- [ 17/100 ] Solving Task: 070dd51e (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 070dd51e finished in 0.99s. Stored 1 solutions.\n\n--- [ 18/100 ] Solving Task: 08ed6ac7 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 08ed6ac7 finished in 0.60s. Stored 1 solutions.\n\n--- [ 19/100 ] Solving Task: 09629e4f (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 09629e4f finished in 2.12s. Stored 1 solutions.\n\n--- [ 20/100 ] Solving Task: 0962bcdd (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0962bcdd finished in 0.80s. Stored 1 solutions.\n\n--- [ 21/100 ] Solving Task: 09c534e7 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 09c534e7 finished in 0.50s. Stored 1 solutions.\n\n--- [ 22/100 ] Solving Task: 0a1d4ef5 (Tier: hard, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0a1d4ef5 finished in 18.88s. Stored 1 solutions.\n\n--- [ 23/100 ] Solving Task: 0a2355a6 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0a2355a6 finished in 0.30s. Stored 1 solutions.\n\n--- [ 24/100 ] Solving Task: 0a938d79 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0a938d79 finished in 1.41s. Stored 1 solutions.\n\n--- [ 25/100 ] Solving Task: 0b148d64 (Tier: hard, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0b148d64 finished in 4.68s. Stored 1 solutions.\n\n--- [ 26/100 ] Solving Task: 0b17323b (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0b17323b finished in 0.29s. Stored 1 solutions.\n\n--- [ 27/100 ] Solving Task: 0bb8deee (Tier: medium, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0bb8deee finished in 1.83s. Stored 1 solutions.\n\n--- [ 28/100 ] Solving Task: 0becf7df (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0becf7df finished in 0.36s. Stored 1 solutions.\n\n--- [ 29/100 ] Solving Task: 0c786b71 (Tier: hard, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0c786b71 finished in 0.97s. Stored 1 solutions.\n\n--- [ 30/100 ] Solving Task: 0c9aba6e (Tier: medium, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0c9aba6e finished in 2.24s. Stored 1 solutions.\n\n--- [ 31/100 ] Solving Task: 0ca9ddb6 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0ca9ddb6 finished in 0.54s. Stored 1 solutions.\n\n--- [ 32/100 ] Solving Task: 0d3d703e (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0d3d703e finished in 0.32s. Stored 1 solutions.\n\n--- [ 33/100 ] Solving Task: 0d87d2a6 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0d87d2a6 finished in 0.67s. Stored 1 solutions.\n\n--- [ 34/100 ] Solving Task: 0e206a2e (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0e206a2e finished in 0.60s. Stored 1 solutions.\n\n--- [ 35/100 ] Solving Task: 0e671a1a (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0e671a1a finished in 0.37s. Stored 1 solutions.\n\n--- [ 36/100 ] Solving Task: 0f63c0b9 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 0f63c0b9 finished in 2.24s. Stored 1 solutions.\n\n--- [ 37/100 ] Solving Task: 103eff5b (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 103eff5b finished in 0.43s. Stored 1 solutions.\n\n--- [ 38/100 ] Solving Task: 10fcaaa3 (Tier: hard, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 10fcaaa3 finished in 0.78s. Stored 1 solutions.\n\n--- [ 39/100 ] Solving Task: 11852cab (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 11852cab finished in 0.80s. Stored 1 solutions.\n\n--- [ 40/100 ] Solving Task: 1190bc91 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1190bc91 finished in 1.92s. Stored 1 solutions.\n\n--- [ 41/100 ] Solving Task: 1190e5a7 (Tier: medium, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1190e5a7 finished in 1.16s. Stored 1 solutions.\n\n--- [ 42/100 ] Solving Task: 11dc524f (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 11dc524f finished in 0.62s. Stored 1 solutions.\n\n--- [ 43/100 ] Solving Task: 11e1fe23 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 11e1fe23 finished in 0.30s. Stored 1 solutions.\n\n--- [ 44/100 ] Solving Task: 12422b43 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 12422b43 finished in 0.57s. Stored 1 solutions.\n\n--- [ 45/100 ] Solving Task: 12997ef3 (Tier: medium, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 2 test case(s)...\n  âž¡ï¸  Task 12997ef3 finished in 1.37s. Stored 2 solutions.\n\n--- [ 46/100 ] Solving Task: 12eac192 (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 12eac192 finished in 0.82s. Stored 1 solutions.\n\n--- [ 47/100 ] Solving Task: 13713586 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 13713586 finished in 2.30s. Stored 1 solutions.\n\n--- [ 48/100 ] Solving Task: 137eaa0f (Tier: medium, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 137eaa0f finished in 1.68s. Stored 1 solutions.\n\n--- [ 49/100 ] Solving Task: 137f0df0 (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 137f0df0 finished in 0.61s. Stored 1 solutions.\n\n--- [ 50/100 ] Solving Task: 13f06aa5 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 13f06aa5 finished in 0.44s. Stored 1 solutions.\n\n--- [ 51/100 ] Solving Task: 140c817e (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 140c817e finished in 1.19s. Stored 1 solutions.\n\n--- [ 52/100 ] Solving Task: 14754a24 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 14754a24 finished in 2.10s. Stored 1 solutions.\n\n--- [ 53/100 ] Solving Task: 1478ab18 (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1478ab18 finished in 0.47s. Stored 1 solutions.\n\n--- [ 54/100 ] Solving Task: 14b8e18c (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 14b8e18c finished in 0.47s. Stored 1 solutions.\n\n--- [ 55/100 ] Solving Task: 150deff5 (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 150deff5 finished in 0.22s. Stored 1 solutions.\n\n--- [ 56/100 ] Solving Task: 15113be4 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 15113be4 finished in 3.37s. Stored 1 solutions.\n\n--- [ 57/100 ] Solving Task: 15660dd6 (Tier: hard, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 15660dd6 finished in 2.09s. Stored 1 solutions.\n\n--- [ 58/100 ] Solving Task: 15663ba9 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 15663ba9 finished in 0.59s. Stored 1 solutions.\n\n--- [ 59/100 ] Solving Task: 15696249 (Tier: hard, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 15696249 finished in 0.80s. Stored 1 solutions.\n\n--- [ 60/100 ] Solving Task: 17829a00 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 17829a00 finished in 2.28s. Stored 1 solutions.\n\n--- [ 61/100 ] Solving Task: 178fcbfb (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 178fcbfb finished in 1.64s. Stored 1 solutions.\n\n--- [ 62/100 ] Solving Task: 17b80ad2 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 17b80ad2 finished in 0.92s. Stored 1 solutions.\n\n--- [ 63/100 ] Solving Task: 17b866bd (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 2 test case(s)...\n  âž¡ï¸  Task 17b866bd finished in 0.42s. Stored 2 solutions.\n\n--- [ 64/100 ] Solving Task: 17cae0c1 (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 17cae0c1 finished in 0.94s. Stored 1 solutions.\n\n--- [ 65/100 ] Solving Task: 18286ef8 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 18286ef8 finished in 1.03s. Stored 1 solutions.\n\n--- [ 66/100 ] Solving Task: 182e5d0f (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 182e5d0f finished in 0.62s. Stored 1 solutions.\n\n--- [ 67/100 ] Solving Task: 18419cfa (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 18419cfa finished in 0.29s. Stored 1 solutions.\n\n--- [ 68/100 ] Solving Task: 18447a8d (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 18447a8d finished in 1.39s. Stored 1 solutions.\n\n--- [ 69/100 ] Solving Task: 184a9768 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 184a9768 finished in 1.86s. Stored 1 solutions.\n\n--- [ 70/100 ] Solving Task: 195ba7dc (Tier: medium, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 195ba7dc finished in 1.78s. Stored 1 solutions.\n\n--- [ 71/100 ] Solving Task: 1990f7a8 (Tier: medium, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1990f7a8 finished in 2.13s. Stored 1 solutions.\n\n--- [ 72/100 ] Solving Task: 19bb5feb (Tier: medium, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 19bb5feb finished in 1.26s. Stored 1 solutions.\n\n--- [ 73/100 ] Solving Task: 1a07d186 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1a07d186 finished in 0.91s. Stored 1 solutions.\n\n--- [ 74/100 ] Solving Task: 1a244afd (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1a244afd finished in 0.81s. Stored 1 solutions.\n\n--- [ 75/100 ] Solving Task: 1a2e2828 (Tier: hard, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1a2e2828 finished in 1.31s. Stored 1 solutions.\n\n--- [ 76/100 ] Solving Task: 1a6449f1 (Tier: hard, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1a6449f1 finished in 8.72s. Stored 1 solutions.\n\n--- [ 77/100 ] Solving Task: 1acc24af (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1acc24af finished in 0.54s. Stored 1 solutions.\n\n--- [ 78/100 ] Solving Task: 1b2d62fb (Tier: medium, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1b2d62fb finished in 1.30s. Stored 1 solutions.\n\n--- [ 79/100 ] Solving Task: 1b59e163 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1b59e163 finished in 1.49s. Stored 1 solutions.\n\n--- [ 80/100 ] Solving Task: 1b60fb0c (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1b60fb0c finished in 0.17s. Stored 1 solutions.\n\n--- [ 81/100 ] Solving Task: 1b8318e3 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1b8318e3 finished in 1.29s. Stored 1 solutions.\n\n--- [ 82/100 ] Solving Task: 1be83260 (Tier: medium, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 2 test case(s)...\n  âž¡ï¸  Task 1be83260 finished in 2.83s. Stored 2 solutions.\n\n--- [ 83/100 ] Solving Task: 1bfc4729 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1bfc4729 finished in 1.01s. Stored 1 solutions.\n\n--- [ 84/100 ] Solving Task: 1c02dbbe (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1c02dbbe finished in 0.43s. Stored 1 solutions.\n\n--- [ 85/100 ] Solving Task: 1c0d0a4b (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1c0d0a4b finished in 1.27s. Stored 1 solutions.\n\n--- [ 86/100 ] Solving Task: 1c56ad9f (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1c56ad9f finished in 0.20s. Stored 1 solutions.\n\n--- [ 87/100 ] Solving Task: 1c786137 (Tier: hard, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1c786137 finished in 3.11s. Stored 1 solutions.\n\n--- [ 88/100 ] Solving Task: 1caeab9d (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1caeab9d finished in 0.55s. Stored 1 solutions.\n\n--- [ 89/100 ] Solving Task: 1cf80156 (Tier: medium, Basin: scaling) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1cf80156 finished in 0.97s. Stored 1 solutions.\n\n--- [ 90/100 ] Solving Task: 1d0a4b61 (Tier: hard, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1d0a4b61 finished in 12.65s. Stored 1 solutions.\n\n--- [ 91/100 ] Solving Task: 1d398264 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 2 test case(s)...\n  âž¡ï¸  Task 1d398264 finished in 0.62s. Stored 2 solutions.\n\n--- [ 92/100 ] Solving Task: 1d61978c (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1d61978c finished in 1.37s. Stored 1 solutions.\n\n--- [ 93/100 ] Solving Task: 1da012fc (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1da012fc finished in 0.84s. Stored 1 solutions.\n\n--- [ 94/100 ] Solving Task: 1e0a9b12 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1e0a9b12 finished in 0.49s. Stored 1 solutions.\n\n--- [ 95/100 ] Solving Task: 1e32b0e9 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1e32b0e9 finished in 0.65s. Stored 1 solutions.\n\n--- [ 96/100 ] Solving Task: 1e5d6875 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1e5d6875 finished in 0.37s. Stored 1 solutions.\n\n--- [ 97/100 ] Solving Task: 1e81d6f9 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1e81d6f9 finished in 2.30s. Stored 1 solutions.\n\n--- [ 98/100 ] Solving Task: 1efba499 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1efba499 finished in 0.79s. Stored 1 solutions.\n\n--- [ 99/100 ] Solving Task: 1f0c79e5 (Tier: easy, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1f0c79e5 finished in 0.64s. Stored 1 solutions.\n\n--- [ 100/100 ] Solving Task: 1f642eb9 (Tier: medium, Basin: color_mapping) ---\n  Querying LTM-v4 (k-NN)...\n  (This is the *real* LTM, not the HPN)\n  LTM-v4 Cache Miss. No similar task found.\n  Executing Phase 3a (Î”H-) Fast-Brain Architect...\n  âš ï¸  Phase 3a (Î”H-) Fast-Brain FAILED: [Playbook.Fail.NoMatch]\n  Executing Phase 3b (Î”H+) Slow-Brain Search...\n  âš ï¸  Phase 3b (Î”H+) Slow-Brain FAILED: [Synthesizer.Fail.MaxDepth]\n  Applying programs to 1 test case(s)...\n  âž¡ï¸  Task 1f642eb9 finished in 0.63s. Stored 1 solutions.\n\n======================================================================\nâœ… MAIN INFERENCE LOOP COMPLETE\n  Total tasks processed: 100 / 100\n  Total Solve Time: 2.41 minutes\n\n======================================================================\nâ±ï¸  DETAILED TIMING BREAKDOWN\n======================================================================\n  SymbolicProgramSynthesizer.solve        :  278.59s ( 200 calls, avg: 1.393s)\n  Synthesizer._precompute_finders         :  200.63s (119810 calls, avg: 0.002s)\n  GameGenie_LTMv4_Training                :  138.30s (   1 calls, avg: 138.300s)\n  PerceptionEngine.analyze                :   76.02s (123698 calls, avg: 0.001s)\n  Primitive.find_objects                  :   71.68s (120616 calls, avg: 0.001s)\n  Phase3b_DeepSearch.0a1d4ef5             :   18.70s (   1 calls, avg: 18.696s)\n  GameGenie.find_program.0a1d4ef5         :   13.70s (   1 calls, avg: 13.700s)\n  GameGenie.find_program.1d0a4b61         :   12.36s (   1 calls, avg: 12.358s)\n  Phase3b_DeepSearch.1d0a4b61             :   12.26s (   1 calls, avg: 12.263s)\n  Phase3b_DeepSearch.1a6449f1             :    8.62s (   1 calls, avg: 8.625s)\n  GameGenie.find_program.1a6449f1         :    8.55s (   1 calls, avg: 8.555s)\n  Primitive.recolor                       :    8.41s (77496 calls, avg: 0.000s)\n  Primitive.move                          :    7.61s (24782 calls, avg: 0.000s)\n  TaskFingerprinter.fingerprint           :    6.86s ( 498 calls, avg: 0.014s)\n  VisionModelEncoder.encode_grid_to_vector:    6.61s (3082 calls, avg: 0.002s)\n  Synthesizer.get_playbook_seeds          :    5.11s ( 200 calls, avg: 0.026s)\n  Phase3b_DeepSearch.0b148d64             :    4.66s (   1 calls, avg: 4.664s)\n  GameGenie.find_program.0b148d64         :    4.66s (   1 calls, avg: 4.657s)\n  HeuristicPlaybookSolver.solve           :    4.31s ( 190 calls, avg: 0.023s)\n  GameGenie.find_program.0607ce86         :    3.94s (   1 calls, avg: 3.944s)\n======================================================================\n\n======================================================================\nðŸŒŠâš›ï¸ Final Validation & Sanitization Pass\n======================================================================\n  Validating submission against all 240 required tasks...\n\n! VALIDATION WARNING: 140 tasks were missing and replaced with fallbacks.\n\nâš ï¸  Submission contains fallbacks or errors, but is now 100% format-compliant.\n\nðŸ’¾ Saving sanitized submission to: /kaggle/working/submission.json\n   âœ… Saved 240 tasks (260.3 KB)\n\n======================================================================\nðŸ†ðŸ NOTEBOOK EXECUTION COMPLETE ðŸðŸ†\n======================================================================\n  Total Tasks in Submission: 240 / 240\n  Total Notebook Runtime: 4.75 minutes\n  (Total Budget: 480.00 minutes)\n\n  Final File: /kaggle/working/submission.json\n  Final Size: 260.3 KB\n  âœ… MetricLogger closed. Final logs saved to /kaggle/working/lucid_metrics.csv\n\n  Ready for Kaggle submission.\n======================================================================\n","output_type":"stream"}],"execution_count":10}]}